{
 "cells": [
  {
   "cell_type": "raw",
   "id": "85fed3fd",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "source": [
    "```\n",
    "22.04.04\n",
    "1. NN / 256-128-64-20 / adam / batch=64  =>  0.6265\n",
    "2. NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128  =>  0.6811 \n",
    "3. NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / Nadam / batch=256  =>  0.6613\n",
    "\n",
    "22.04.05\n",
    "1. NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=256  =>  0.6798\n",
    "2. KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128  =>  0.7337 \n",
    "3. KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / tanh / adam / batch=128  =>  0.7328\n",
    "\n",
    "22.04.06\n",
    "1. 불용어 처리 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128 / 0.70  =>  0.7391\n",
    "\n",
    "22.04.07\n",
    "1. 불용어 처리, ngram(1,2) / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128  =>  0.6845\n",
    "2. 불용어 처리/ countvectorizer 옵션 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128 / 0.7068 =>  0.7417 \n",
    "3. 불용어 처리/ countvectorizer 옵션 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / rmsprop / batch=128 / 0.70  => 0.7359\n",
    "\n",
    "22.04.08\n",
    "1. 불용어 처리/ countvectorizer 옵션 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / rlrp / batch=128 / 0.70  => 0.7426\n",
    "2. 불용어 / tfidf 옵션 / mlp nb lgbm voting  => 0.6891\n",
    "\n",
    "22.04.11\n",
    "1. bert_sklearn => 0.7042\n",
    "2. bert_sklearn => 0.7045\n",
    "3. 불용어 / tfidf 옵션 / mlp nb voting  => 0.7287\n",
    "\n",
    "22.04.12\n",
    "1. 불용어 + mlp => 0.7259\n",
    "\n",
    "22.04.13\n",
    "1. mlp nb voting => 0.7469 \n",
    "\n",
    "22.04.14\n",
    "1. sgd => 0.7588 (best)\n",
    "2. passive => 0.7560\n",
    "\n",
    "22.04.15\n",
    "1. sgd(ngram 1,2) => 0.7664\n",
    "2. sgd(ngram 1,3 , alpha=1e-4) => 0.7668\n",
    "3. sgd(ngram 1,2 , sublinear) => 0.7844\n",
    "```\n",
    "mlp만.\n",
    "베이스라인 + mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "임베딩 알고리즘 변경\n",
    "분류 알고리즘 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbed0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab7e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4f730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f4130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(texts): \n",
    "  corpus = [] \n",
    "  for i in range(0, len(texts)): \n",
    "\n",
    "    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n",
    "    review = re.sub(r'\\d+','', review)#숫자 제거\n",
    "    review = review.lower() #소문자 변환\n",
    "    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n",
    "    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n",
    "    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n",
    "    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n",
    "    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n",
    "    review = re.sub(r'_', ' ', review) #space from the end 제거\n",
    "    corpus.append(review) \n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0bc1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they were and even if washington might conside...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we run spacenews views on our stareach bbs a l...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not to worry the masons have been demonized an...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>only brendan mckay or maybe arf would come to ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help i am running some sample problems from or...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  they were and even if washington might conside...      10\n",
       "1  we run spacenews views on our stareach bbs a l...      14\n",
       "2  not to worry the masons have been demonized an...      19\n",
       "3  only brendan mckay or maybe arf would come to ...      17\n",
       "4  help i am running some sample problems from or...       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = clean_text(train['text']) #메소드 적용\n",
    "train['text'] = temp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d989321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the vlide adapter can be much faster then the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah in a fire that reportedly burned hotter t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>judge i grant you immunity from whatever may b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i too put a corbin seat on my hawk i got the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do i ever after years of having health problem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  the vlide adapter can be much faster then the ...\n",
       "1  yeah in a fire that reportedly burned hotter t...\n",
       "2  judge i grant you immunity from whatever may b...\n",
       "3  i too put a corbin seat on my hawk i got the s...\n",
       "4  do i ever after years of having health problem..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = clean_text(test['text']) #메소드 적용\n",
    "test['text'] = temp\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa0c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['text']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b418ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       they were and even if washington might conside...\n",
       "1       we run spacenews views on our stareach bbs a l...\n",
       "2       not to worry the masons have been demonized an...\n",
       "3       only brendan mckay or maybe arf would come to ...\n",
       "4       help i am running some sample problems from or...\n",
       "                              ...                        \n",
       "9228    precisely why not cuba why not the hatians are...\n",
       "9229    your custom resume on disk macintosh or ibm co...\n",
       "9230    throughout the years of the israelarabpalestin...\n",
       "9231    does anyone know if there are any devices avai...\n",
       "9232    give me a break chum are you telling me that c...\n",
       "Name: text, Length: 9233, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49519491",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131931cc",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72618490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2033064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97532c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test.text #문서 데이터 생성\n",
    "\n",
    "test_X_vect = vectorizer.transform(test_X) #문서 데이터 transform \n",
    "#test 데이터를 대상으로 fit_transform 메소드를 실행하는 것은 test 데이터를 활용해 vectorizer 를 학습 시키는 것으롤 data leakage 에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e76fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4395b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1bc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d55a29",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96be100",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3545e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de42959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e908bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "581b74e4",
   "metadata": {},
   "source": [
    "# 알고리즘 실험실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e46abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "641b63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse = False)\n",
    "y = ohe.fit_transform(train[['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=train_X.shape[1], activation = 'elu'))\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8837a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, y, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda00f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = np.argmax(pred, axis = 1)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission_ngram(1,2).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a9a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73426b98",
   "metadata": {},
   "source": [
    "# Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda1d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77eadbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X[tr_idx], y[tr_idx]\n",
    "    val_x, val_y = train_X[val_idx], y[val_idx]\n",
    "    \n",
    "    ### NN 모델\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=train_X.shape[1], activation = 'elu'))\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    result = model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test_X_vect) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(nn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = np.argmax(nn_pred, axis = 1)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d45ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c5387d1",
   "metadata": {},
   "source": [
    "## 불용어처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(texts): \n",
    "  corpus = [] \n",
    "  for i in range(0, len(texts)): \n",
    "\n",
    "    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n",
    "    review = re.sub(r'\\d+','', review)#숫자 제거\n",
    "    review = review.lower() #소문자 변환\n",
    "    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n",
    "    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n",
    "    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n",
    "    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n",
    "    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n",
    "    review = re.sub(r'_', ' ', review) #space from the end 제거\n",
    "    corpus.append(review) \n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(train['text']) #메소드 적용\n",
    "train['text'] = temp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d730528",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(test['text']) #메소드 적용\n",
    "test['text'] = temp\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee63a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['text']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58a468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_X)):\n",
    "    a = train_X[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    train_X[i] = result\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test.text)):\n",
    "    a = test.text[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    test.text[i] = result\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b019cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d99555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d13fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test.text #문서 데이터 생성\n",
    "\n",
    "test_X_vect = vectorizer.transform(test_X) #문서 데이터 transform \n",
    "#test 데이터를 대상으로 fit_transform 메소드를 실행하는 것은 test 데이터를 활용해 vectorizer 를 학습 시키는 것으롤 data leakage 에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e473e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e51093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 6, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5efd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X[tr_idx], y[tr_idx]\n",
    "    val_x, val_y = train_X[val_idx], y[val_idx]\n",
    "    \n",
    "    ### NN 모델\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=train_X.shape[1], activation = 'elu'))\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    result = model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test_X_vect) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa756d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(nn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca199c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = np.argmax(nn_pred, axis = 1)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37681263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b52f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1cea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad3f703",
   "metadata": {},
   "source": [
    "## 불용어 + Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd363034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ee9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "vectorizer.fit(np.array(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_vec = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd2a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "vectorizer.fit(np.array(train[\"text\"]))\n",
    "\n",
    "train_vec = vectorizer.transform(train[\"text\"])\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_vec = vectorizer.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2525f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545d98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6896e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1a5955",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12188/4093190040.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ff78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X_vec, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X_vec[tr_idx], y[tr_idx]\n",
    "    val_x, val_y = train_X_vec[val_idx], y[val_idx]\n",
    "    \n",
    "    ### NN 모델\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=train_X_vec.shape[1], activation = 'elu'))\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test_vec) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d77182",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5001a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90b5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5817bd66",
   "metadata": {},
   "source": [
    "## 토크나이저 + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe34d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a49c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
    "print(X_train_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdfe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 정수가 어떤 단어에 부여되었는지 확인해봅시다.\n",
    "word_to_index = tokenizer.word_index\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(word_to_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) + 1\n",
    "print('단어 집합의 크기: {}'.format((vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n",
    "print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n",
    "plt.hist([len(sample) for sample in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 6737\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
    "print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cdf32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_units = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=6737))\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_padded, y, epochs=4, batch_size=256, validation_split=0.3)\n",
    "\n",
    "# 너무 오래걸리고 정확도도 낮음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287c2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b65204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074c3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d888e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3322091",
   "metadata": {},
   "source": [
    "## 워드투벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab642b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# countvectorizer 사용 9233 * 143522\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd807bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8706b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = sent_tokenize(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ceae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9055bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word_tokenize(sent_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a66b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c48b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X_token = []\n",
    "for i in range(len(train_X)):\n",
    "    try:\n",
    "        sent_text = sent_tokenize(train_X[i])\n",
    "        result = word_tokenize(sent_text[0])\n",
    "        train_X_token.append(result)\n",
    "    except:\n",
    "        print(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b615484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf23650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc8f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d01e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c36a142",
   "metadata": {},
   "source": [
    "## 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(texts): \n",
    "  corpus = [] \n",
    "  for i in range(0, len(texts)): \n",
    "\n",
    "    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n",
    "    review = re.sub(r'\\d+','', review)#숫자 제거\n",
    "    review = review.lower() #소문자 변환\n",
    "    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n",
    "    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n",
    "    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n",
    "    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n",
    "    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n",
    "    review = re.sub(r'_', ' ', review) #space from the end 제거\n",
    "    corpus.append(review) \n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97aef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(train['text']) #메소드 적용\n",
    "train['text'] = temp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(test['text']) #메소드 적용\n",
    "test['text'] = temp\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['text']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4be77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e172b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dcd0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_X)):\n",
    "    a = train_X[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    train_X[i] = result\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test.text)):\n",
    "    a = test.text[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    test.text[i] = result\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe0bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words = 'english')\n",
    "\n",
    "vectorizer.fit(np.array(train[\"text\"]))\n",
    "\n",
    "train_vec = vectorizer.transform(train[\"text\"])\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_vec = vectorizer.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50537ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier()\n",
    "NB = MultinomialNB()\n",
    "LGBM = LGBMClassifier()\n",
    "\n",
    "VC = VotingClassifier(estimators=[('mlp',MLP),('nb',NB),('lgbm',LGBM)],voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VC.fit(train_vec,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(train_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11383a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179891fd",
   "metadata": {},
   "source": [
    "## bertclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_sklearn import BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e77bf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Fold Training.....\n",
      "Building sklearn text classifier...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'validation_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12188/1807891508.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'model_{i + 1}.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m### 최고 성능 기록 모델 Load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'validation_data'"
     ]
    }
   ],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((train_y.shape[0], 20 ))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X[tr_idx], train_y[tr_idx]\n",
    "    val_x, val_y = train_X[val_idx], train_y[val_idx]\n",
    "    \n",
    "    model = BertClassifier(bert_model=\"bert-large-cased\")\n",
    "    \n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test.text) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb65b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7480800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-large-cased model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12188/1723167297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, load_at_start)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload_at_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_bert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;31m# to fix BatchLayer1D prob in rare case last batch is a singlton w MLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\sklearn.py\u001b[0m in \u001b[0;36mload_bert\u001b[1;34m(self, state_dict)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# load a vanilla bert model ready to finetune:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;31m# pretrained bert LM + untrained classifier/regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         self.model = get_model(bert_model=self.bert_model,\n\u001b[0m\u001b[0;32m    239\u001b[0m                                \u001b[0mbert_config_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_config_json\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                                \u001b[0mfrom_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\model\\utils.py\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(bert_model, bert_config_json, from_tf, num_labels, model_type, num_mlp_layers, num_mlp_hiddens, state_dict, local_rank)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# Load from pre-trained model archive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading %s model...\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         model = BertPlusMLP.from_pretrained(bert_model,\n\u001b[0m\u001b[0;32m     95\u001b[0m                                             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                                             \u001b[0mstate_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\modeling.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m         return cls.from_model_ckpt(config_path, weights_path, state_dict,\n\u001b[0m\u001b[0;32m    708\u001b[0m                                    from_tf=from_tf, tempdir=tempdir, *inputs, **kwargs)\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\modeling.py\u001b[0m in \u001b[0;36mfrom_model_ckpt\u001b[1;34m(cls, config_file_or_dict, weights_path, state_dict, from_tf, tempdir, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         \u001b[1;31m# Instantiate model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[1;31m# If this is a Pytorch checkpoint, load model into state_dict if it has not been passed in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\model\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, model_type, num_labels, num_mlp_layers, num_mlp_hiddens)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\modeling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, output_attentions, keep_multihead_output)\u001b[0m\n\u001b[0;32m    924\u001b[0m                                            keep_multihead_output=keep_multihead_output)\n\u001b[0;32m    925\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertPooler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_bert_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprune_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads_to_prune\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\modeling.py\u001b[0m in \u001b[0;36minit_bert_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;31m# Slightly different from the TF version which uses truncated_normal for initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[1;31m# cf https://github.com/pytorch/pytorch/pull/5617\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertLayerNorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c807ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f208af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10483c56",
   "metadata": {},
   "source": [
    "## sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5e7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d9ad61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.text # train에서 문서 추출\n",
    "y = train.target # train에서 라벨 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2c42c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english',sublinear_tf=True)\n",
    "vectorizer.fit(X) # countvectorizer 학습\n",
    "X = vectorizer.transform(X) # transform"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c6f4f02",
   "metadata": {},
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "svc =  LinearSVC(loss='hinge', penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3c5b2cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(loss='squared_hinge', max_iter=5,\n",
       "                               random_state=42))])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='squared_hinge', penalty='l2',alpha=1e-4, random_state=42, max_iter=5)),\n",
    "                 ])\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "76b05c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test.text # 문서 데이터 생성\n",
    "test_X_vect = vectorizer.transform(test_X) # 문서 데이터 transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d015560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 16 11 ...  4  0 12]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_X_vect)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "82de2fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 16, 11, ...,  4,  0, 12], dtype=int64)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f92268b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = pred\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission_sgd_ngram(1,2)+options.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb72bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e313d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b0886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce26c6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04c9319b",
   "metadata": {},
   "source": [
    "# 테스트셋 나눠서 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb0aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd57c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.text, train.target, test_size = 0.2, shuffle=True, stratify=train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34356782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef10537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english',sublinear_tf=True)\n",
    "vectorizer.fit(X_train) # countvectorizer 학습\n",
    "X = vectorizer.transform(X_train) # transform"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a037d15",
   "metadata": {},
   "source": [
    "sgd = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "svc =  LinearSVC(loss='hinge', penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82701154",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_vect = vectorizer.transform(X_test) # 문서 데이터 transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adfbed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "model = Pipeline([('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='squared_hinge', penalty='l2',alpha=1e-4, random_state=42, max_iter=5))\n",
    "                  #('svc', LinearSVC(loss='hinge', penalty='l2'))\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b8c079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 SGDClassifier(loss='squared_hinge', max_iter=5,\n",
       "                               random_state=42))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82601c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 12  8 ...  8 16 13]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_X_vect)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fa86b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12,  8, ...,  8, 16, 13], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "50ad90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.7845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "248858c2",
   "metadata": {},
   "source": [
    "1,2 => 0.7482\n",
    "1,3 => 0.7493\n",
    "알파 => 1e-4\n",
    "squared_hinge => 0.7537\n",
    "stopword(1,3) => 0.7650\n",
    "stopword(1,2) => 0.7699\n",
    "sublinear_tf => 0.7791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c57788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
