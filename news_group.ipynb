{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d386beba",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "source": [
    "```\n",
    "22.04.04\n",
    "1. NN / 256-128-64-20 / adam / batch=64  =>  0.6265\n",
    "2. NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128  =>  0.6811 \n",
    "3. NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / Nadam / batch=256  =>  0.6613\n",
    "\n",
    "22.04.05\n",
    "1. NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=256  =>  0.6798\n",
    "2. KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128  =>  0.7337 \n",
    "3. KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / tanh / adam / batch=128  =>  0.7328\n",
    "\n",
    "22.04.06\n",
    "1. 불용어 처리 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128 / 0.70  =>  0.7391\n",
    "\n",
    "22.04.07\n",
    "1. 불용어 처리, ngram(1,2) / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128  =>  0.6845\n",
    "2. 불용어 처리/ countvectorizer 옵션 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / batch=128 / 0.7068 =>  0.7417 \n",
    "3. 불용어 처리/ countvectorizer 옵션 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / rmsprop / batch=128 / 0.70  => 0.7359\n",
    "\n",
    "22.04.08\n",
    "1. 불용어 처리/ countvectorizer 옵션 / KFold / NN / 256-128(Drop 0.5)-64(Drop 0.5)-20 / elu / adam / rlrp / batch=128 / 0.70  => 0.7426(best)\n",
    "2. 불용어 / tfidf 옵션 / mlp nb lgbm voting  => 0.6891\n",
    "\n",
    "22.04.11\n",
    "1. bert_sklearn => 0.7042\n",
    "2. bert_sklearn => 0.7045\n",
    "3. 불용어 / tfidf 옵션 / mlp nb voting  => 0.7287\n",
    "```\n",
    "count vs tfidf -> tfidf적용후 NN이 안돼 -> shape문제?\n",
    "RNN 시도 -> 느림\n",
    "LSTM 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "임베딩 알고리즘 변경\n",
    "분류 알고리즘 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbed0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f4130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(texts): \n",
    "  corpus = [] \n",
    "  for i in range(0, len(texts)): \n",
    "\n",
    "    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n",
    "    review = re.sub(r'\\d+','', review)#숫자 제거\n",
    "    review = review.lower() #소문자 변환\n",
    "    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n",
    "    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n",
    "    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n",
    "    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n",
    "    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n",
    "    review = re.sub(r'_', ' ', review) #space from the end 제거\n",
    "    corpus.append(review) \n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(train['text']) #메소드 적용\n",
    "train['text'] = temp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d989321",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(test['text']) #메소드 적용\n",
    "test['text'] = temp\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['text']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b418ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49519491",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131931cc",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72618490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2033064",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97532c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test.text #문서 데이터 생성\n",
    "\n",
    "test_X_vect = vectorizer.transform(test_X) #문서 데이터 transform \n",
    "#test 데이터를 대상으로 fit_transform 메소드를 실행하는 것은 test 데이터를 활용해 vectorizer 를 학습 시키는 것으롤 data leakage 에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e76fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4395b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1bc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d55a29",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dc14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96be100",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3545e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de42959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e908bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "581b74e4",
   "metadata": {},
   "source": [
    "# 알고리즘 실험실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e46abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse = False)\n",
    "y = ohe.fit_transform(train[['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=train_X.shape[1], activation = 'elu'))\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8837a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, y, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_X_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda00f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = np.argmax(pred, axis = 1)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission_ngram(1,2).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a9a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73426b98",
   "metadata": {},
   "source": [
    "# Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eadbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca4d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X[tr_idx], y[tr_idx]\n",
    "    val_x, val_y = train_X[val_idx], y[val_idx]\n",
    "    \n",
    "    ### NN 모델\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=train_X.shape[1], activation = 'elu'))\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    result = model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test_X_vect) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(nn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = np.argmax(nn_pred, axis = 1)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d45ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c5387d1",
   "metadata": {},
   "source": [
    "## 불용어처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e238d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(texts): \n",
    "  corpus = [] \n",
    "  for i in range(0, len(texts)): \n",
    "\n",
    "    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n",
    "    review = re.sub(r'\\d+','', review)#숫자 제거\n",
    "    review = review.lower() #소문자 변환\n",
    "    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n",
    "    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n",
    "    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n",
    "    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n",
    "    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n",
    "    review = re.sub(r'_', ' ', review) #space from the end 제거\n",
    "    corpus.append(review) \n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(train['text']) #메소드 적용\n",
    "train['text'] = temp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d730528",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(test['text']) #메소드 적용\n",
    "test['text'] = temp\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee63a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['text']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58a468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_X)):\n",
    "    a = train_X[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    train_X[i] = result\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test.text)):\n",
    "    a = test.text[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    test.text[i] = result\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205919c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b019cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d99555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.inverse_transform(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d13fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test.text #문서 데이터 생성\n",
    "\n",
    "test_X_vect = vectorizer.transform(test_X) #문서 데이터 transform \n",
    "#test 데이터를 대상으로 fit_transform 메소드를 실행하는 것은 test 데이터를 활용해 vectorizer 를 학습 시키는 것으롤 data leakage 에 해당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e473e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e51093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 6, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5efd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X[tr_idx], y[tr_idx]\n",
    "    val_x, val_y = train_X[val_idx], y[val_idx]\n",
    "    \n",
    "    ### NN 모델\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=train_X.shape[1], activation = 'elu'))\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    result = model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test_X_vect) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa756d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(nn_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca199c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['target'] = np.argmax(nn_pred, axis = 1)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37681263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b52f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1cea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad3f703",
   "metadata": {},
   "source": [
    "## 불용어 + Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd363034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ee9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "vectorizer.fit(np.array(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_vec = vectorizer.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd2a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "vectorizer.fit(np.array(train[\"text\"]))\n",
    "\n",
    "train_vec = vectorizer.transform(train[\"text\"])\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_vec = vectorizer.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2525f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545d98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6896e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 10, random_state = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ff78d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_acc = []\n",
    "nn_pred = np.zeros((y.shape[0], y.shape[1]))\n",
    "\n",
    "for i, (tr_idx, val_idx) in enumerate(skf.split(train_X_vec, train_y)) :\n",
    "    print(f'{i + 1} Fold Training.....')\n",
    "    tr_x, tr_y = train_X_vec[tr_idx], y[tr_idx]\n",
    "    val_x, val_y = train_X_vec[val_idx], y[val_idx]\n",
    "    \n",
    "    ### NN 모델\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=train_X_vec.shape[1], activation = 'elu'))\n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_accuracy', mode = 'max', verbose = 0)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 10, batch_size = 128, callbacks = [mc], verbose = 1)\n",
    "\n",
    "    ### 최고 성능 기록 모델 Load\n",
    "    best = load_model(f'model_{i + 1}.h5')\n",
    "    ### validation predict\n",
    "    val_pred = best.predict(val_x)\n",
    "    ### 확률값 중 최대값을 클래스로 매칭\n",
    "    val_cls = np.argmax(val_pred, axis = 1)\n",
    "    ### Fold별 val_mae 산출\n",
    "    fold_nn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
    "    nn_acc.append(fold_nn_acc)\n",
    "    print(f'{i + 1} Fold nn acc = {fold_nn_acc}\\n')\n",
    "\n",
    "    ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
    "    fold_pred = best.predict(test_vec) / skf.n_splits\n",
    "    nn_pred += fold_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d77182",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf94af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5001a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90b5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5817bd66",
   "metadata": {},
   "source": [
    "## 토크나이저 + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe34d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a49c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
    "print(X_train_encoded[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdfe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 정수가 어떤 단어에 부여되었는지 확인해봅시다.\n",
    "word_to_index = tokenizer.word_index\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(word_to_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) + 1\n",
    "print('단어 집합의 크기: {}'.format((vocab_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n",
    "print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n",
    "plt.hist([len(sample) for sample in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca8924",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 6737\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
    "print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cdf32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "embedding_dim = 32\n",
    "hidden_units = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=6737))\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train_padded, y, epochs=4, batch_size=256, validation_split=0.3)\n",
    "\n",
    "# 너무 오래걸리고 정확도도 낮음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287c2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b65204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074c3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d888e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3322091",
   "metadata": {},
   "source": [
    "## 워드투벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab642b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# countvectorizer 사용 9233 * 143522\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd807bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8706b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = sent_tokenize(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ceae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9055bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word_tokenize(sent_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a66b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c48b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X_token = []\n",
    "for i in range(len(train_X)):\n",
    "    try:\n",
    "        sent_text = sent_tokenize(train_X[i])\n",
    "        result = word_tokenize(sent_text[0])\n",
    "        train_X_token.append(result)\n",
    "    except:\n",
    "        print(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b615484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf23650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc8f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d01e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c36a142",
   "metadata": {},
   "source": [
    "## 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def clean_text(texts): \n",
    "  corpus = [] \n",
    "  for i in range(0, len(texts)): \n",
    "\n",
    "    review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"\\n\\]\\[\\>]', '',texts[i]) #@%*=()/+ 와 같은 문장부호 제거\n",
    "    review = re.sub(r'\\d+','', review)#숫자 제거\n",
    "    review = review.lower() #소문자 변환\n",
    "    review = re.sub(r'\\s+', ' ', review) #extra space 제거\n",
    "    review = re.sub(r'<[^>]+>','',review) #Html tags 제거\n",
    "    review = re.sub(r'\\s+', ' ', review) #spaces 제거\n",
    "    review = re.sub(r\"^\\s+\", '', review) #space from start 제거\n",
    "    review = re.sub(r'\\s+$', '', review) #space from the end 제거\n",
    "    review = re.sub(r'_', ' ', review) #space from the end 제거\n",
    "    corpus.append(review) \n",
    "  \n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97aef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(train['text']) #메소드 적용\n",
    "train['text'] = temp\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clean_text(test['text']) #메소드 적용\n",
    "test['text'] = temp\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['text']\n",
    "train_y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4be77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e172b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dcd0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_X)):\n",
    "    a = train_X[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    train_X[i] = result\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test.text)):\n",
    "    a = test.text[i].split(' ')\n",
    "    result = ''\n",
    "    for word in a: \n",
    "        if word not in stop_words: \n",
    "            result = result + ' ' + word \n",
    "    test.text[i] = result\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe0bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words = 'english')\n",
    "\n",
    "vectorizer.fit(np.array(train[\"text\"]))\n",
    "\n",
    "train_vec = vectorizer.transform(train[\"text\"])\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "test_vec = vectorizer.transform(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50537ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0034e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier()\n",
    "NB = MultinomialNB()\n",
    "LGBM = LGBMClassifier()\n",
    "\n",
    "VC = VotingClassifier(estimators=[('mlp',MLP),('nb',NB),('lgbm',LGBM)],voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VC.fit(train_vec,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(train_vec, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11383a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "179891fd",
   "metadata": {},
   "source": [
    "## bertclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_sklearn import BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb65b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertClassifier(bert_model=\"bert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7480800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75eab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
