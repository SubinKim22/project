{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7baf49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d40be4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# textrank\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def textrank(article):\n",
    "    korean_data = article\n",
    "    \n",
    "    text = re.sub(r\"\\n+\", \" \", korean_data)\n",
    "    sentences = re.split(\"[\\.?!]\\s+\", text)\n",
    "    komoran = Komoran()\n",
    "    \n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        if(sentence == \"\" or len(sentence) == 0):\n",
    "            continue\n",
    "        temp_dict = dict()\n",
    "        temp_dict['sentence'] = sentence\n",
    "        temp_dict['token_list'] = komoran.nouns(sentence)\n",
    "        \n",
    "        data.append(temp_dict)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    similarity_matrix = []\n",
    "    for i, row_i in df.iterrows():\n",
    "        i_row_vec = []\n",
    "        for j, row_j in df.iterrows():\n",
    "            if i == j:\n",
    "                i_row_vec.append(0.0)\n",
    "            else:\n",
    "                intersection = len(set(row_i['token_list']) & set(row_j['token_list']))\n",
    "                log_i = math.log(len(set(row_i['token_list'])))\n",
    "                log_j = math.log(len(set(row_j['token_list'])))\n",
    "                similarity = intersection / (log_i + log_j)\n",
    "                i_row_vec.append(similarity)\n",
    "        similarity_matrix.append(i_row_vec)\n",
    "        \n",
    "    weightedGraph = np.array(similarity_matrix)\n",
    "    R = pagerank(weightedGraph)\n",
    "    R = R.sum(axis=1)\n",
    "    indexs = R.argsort()[-3:] # 여기가 세줄요약 느낌 / -5주면 5줄나옴 \n",
    "    \n",
    "    article_summary_str = ''\n",
    "    for index in sorted(indexs):\n",
    "        #print(df['sentence'][index])\n",
    "        #print()\n",
    "        article_summary = df['sentence'][index]\n",
    "        article_summary_str += ' ' + article_summary\n",
    "        \n",
    "    return article_summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e486d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "import torch\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f57f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gogamza(article):\n",
    "    input_ids = tokenizer.encode(article)\n",
    "    input_ids = [tokenizer.bos_token_id] + input_ids + [tokenizer.eos_token_id]\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "    \n",
    "    summary_text_ids = model.generate(\n",
    "        input_ids = input_ids,\n",
    "        bos_token_id = model.config.bos_token_id,\n",
    "        eos_token_id = model.config.eos_token_id,\n",
    "        length_penalty=2.0, # 길이에 대한 penalty 값\n",
    "        max_length = 150, # 요약문의 최대 길이 설정\n",
    "        min_length = 50, # 요약문의 최소 길이 설정\n",
    "        num_beams = 5) # 문장 생성 시 다음 단어를 탐색하는 영역의 개수\n",
    "    \n",
    "    return tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaaf88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d83e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4b6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import telegram\n",
    "from telegram.ext import Updater\n",
    "from telegram.ext import MessageHandler, Filters\n",
    "import telepot\n",
    "from telepot.loop import MessageLoop # 봇 구동\n",
    "from telepot.namedtuple import InlineKeyboardMarkup as MU # 마크업\n",
    "from telepot.namedtuple import InlineKeyboardButton as BT # 버튼\n",
    "\n",
    "token = '' # 텔레그램 봇 API\n",
    "mc = '' # 본인 텔레그램 숫자 id\n",
    "\n",
    "updater = Updater(token=token, use_context=True)\n",
    "\n",
    "bot = telepot.Bot(token) # 봇 객체 생성\n",
    "\n",
    "bot.sendMessage(chat_id=mc, text='시작')\n",
    "\n",
    "user_text = bot.getUpdates()[-1]['message']['text']\n",
    "\n",
    "def btn_show(msg):\n",
    "    btn1 = BT(text = \"1. textrank(긴 글 세줄요약)\", callback_data = \"1\")\n",
    "    btn2 = BT(text = \"2. gogamza(짧은 글 한줄요약)\", callback_data = \"2\")\n",
    "    mu = MU(inline_keyboard = [[btn1, btn2]])\n",
    "    bot.sendMessage(mc, \"요약기를 선택하세요\", reply_markup = mu)\n",
    "\n",
    "def query_ans(msg):\n",
    "    query_data = msg[\"data\"]\n",
    "    if query_data == \"1\":\n",
    "        bot.sendMessage(mc, text = textrank(user_text))\n",
    "    elif query_data == \"2\":\n",
    "        bot.sendMessage(mc, text = gogamza(user_text))\n",
    "\n",
    "MessageLoop(bot, {'chat': btn_show, \"callback_query\" : query_ans}).run_as_thread()# 답장 보내기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
