{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "22.03.21\n",
    "1. pycaret(전처리X, top10) => 0.15194\n",
    "\n",
    "22.03.23\n",
    "1. dense(16-32-64-32-16-1) / elu / adam / es = 50 / rlrp = 0.2 20 / epo = 1000 / val_mae = 1.54  => 0.1454\n",
    "2. dense(16-32-64-32-16-1) / elu / Nadam / es = 50 / rlrp = 0.2 45 / epo = 1000 / val_mae = 1.5246  => 0.1439 (best)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "+ cv 나눠서 평균으로 해보기 (손동작 참고)\n",
    "+ 전처리 - 이상치 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42309008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "df2ef4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(mpl.__version__)\n",
    "# seaborn 스타일 사용\n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "# matplotlib 그래프 한글폰트 깨질 때 대처(Mac & Window)\n",
    "import platform\n",
    "if platform.system() == 'Windows':\n",
    "# 윈도우인 경우\n",
    "    font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "    rc('font', family=font_name)\n",
    "else:    \n",
    "# Mac 인 경우\n",
    "    rc('font', family='AppleGothic')\n",
    "    \n",
    "# 그래프에서 마이너스 기호가 표시되도록 하는 설정\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "88b60fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)\n",
    "\n",
    "train_X = train.drop('Target', axis=1)\n",
    "train_y = train.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6685785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.get_dummies(data = train_X, columns = ['Gender'], prefix = 'Gender')\n",
    "test = pd.get_dummies(data = test, columns = ['Gender'], prefix = 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9833928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "for i in range(1,9):\n",
    "    plt.subplot(3,4,i)\n",
    "    sns.distplot(train.iloc[:,i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2178b",
   "metadata": {},
   "source": [
    "# 파이캐럿 (전처리 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pycaret_data = pd.get_dummies(data = train, columns = ['Gender'], prefix = 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f66085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eef8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_clf = setup(data = pycaret_data, target = 'Target', fold_shuffle=True, use_gpu=True, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = compare_models(sort='MAE', n_select=10, fold = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e238e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_top10 = [tune_model(i, fold = 5) for i in top10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14581ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_top10 = blend_models(estimator_list=tuned_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5892149",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(blender_top10)\n",
    "prediction = predict_model(final_model, data= test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00487c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['Target'] = prediction['Label']\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6844e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127081d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35ce8140",
   "metadata": {},
   "source": [
    "# 딥러닝 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "960d3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bce067ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 선언\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=10, activation='elu'))\n",
    "model.add(Dense(32, activation='elu'))    \n",
    "model.add(Dense(64, activation='elu'))            \n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='Nadam', \n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4fa06e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "# 모델 업데이트 및 저장\n",
    "cp = ModelCheckpoint(filepath=modelpath, monitor='val_mae', verbose=0, save_best_only=True, mode = 'min')\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='val_mae', patience=50, mode='min')\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=45, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "26925bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "28/28 [==============================] - 1s 5ms/step - loss: 8.0692 - mae: 8.0692 - val_loss: 4.9444 - val_mae: 4.9444\n",
      "Epoch 2/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 2.3366 - mae: 2.3366 - val_loss: 1.9839 - val_mae: 1.9839\n",
      "Epoch 3/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7231 - mae: 1.7231 - val_loss: 1.9345 - val_mae: 1.9345\n",
      "Epoch 4/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7162 - mae: 1.7162 - val_loss: 1.9255 - val_mae: 1.9255\n",
      "Epoch 5/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7017 - mae: 1.7017 - val_loss: 1.9196 - val_mae: 1.9196\n",
      "Epoch 6/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6952 - mae: 1.6952 - val_loss: 1.9514 - val_mae: 1.9514\n",
      "Epoch 7/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.6941 - mae: 1.6941 - val_loss: 1.9112 - val_mae: 1.9112\n",
      "Epoch 8/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.6875 - mae: 1.6875 - val_loss: 1.9060 - val_mae: 1.9060\n",
      "Epoch 9/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6861 - mae: 1.6861 - val_loss: 1.9055 - val_mae: 1.9055\n",
      "Epoch 10/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6857 - mae: 1.6857 - val_loss: 1.9002 - val_mae: 1.9002\n",
      "Epoch 11/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6754 - mae: 1.6754 - val_loss: 1.9145 - val_mae: 1.9145\n",
      "Epoch 12/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6731 - mae: 1.6731 - val_loss: 1.8975 - val_mae: 1.8975\n",
      "Epoch 13/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6702 - mae: 1.6702 - val_loss: 1.9112 - val_mae: 1.9112\n",
      "Epoch 14/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.6627 - mae: 1.6627 - val_loss: 1.9192 - val_mae: 1.9192\n",
      "Epoch 15/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6703 - mae: 1.6703 - val_loss: 1.8802 - val_mae: 1.8802\n",
      "Epoch 16/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6598 - mae: 1.6598 - val_loss: 1.8796 - val_mae: 1.8796\n",
      "Epoch 17/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6661 - mae: 1.6661 - val_loss: 1.8724 - val_mae: 1.8724\n",
      "Epoch 18/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6572 - mae: 1.6572 - val_loss: 1.8783 - val_mae: 1.8783\n",
      "Epoch 19/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6598 - mae: 1.6598 - val_loss: 1.8765 - val_mae: 1.8765\n",
      "Epoch 20/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6471 - mae: 1.6471 - val_loss: 1.8923 - val_mae: 1.8923\n",
      "Epoch 21/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.6493 - mae: 1.6493 - val_loss: 1.8605 - val_mae: 1.8605\n",
      "Epoch 22/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6444 - mae: 1.6444 - val_loss: 1.9049 - val_mae: 1.9049\n",
      "Epoch 23/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.6363 - mae: 1.6363 - val_loss: 1.8432 - val_mae: 1.8432\n",
      "Epoch 24/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6268 - mae: 1.6268 - val_loss: 1.8508 - val_mae: 1.8508\n",
      "Epoch 25/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.6188 - mae: 1.6188 - val_loss: 1.8213 - val_mae: 1.8213\n",
      "Epoch 26/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6027 - mae: 1.6027 - val_loss: 1.8147 - val_mae: 1.8147\n",
      "Epoch 27/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6049 - mae: 1.6049 - val_loss: 1.7934 - val_mae: 1.7934\n",
      "Epoch 28/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5752 - mae: 1.5752 - val_loss: 1.7983 - val_mae: 1.7983\n",
      "Epoch 29/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5859 - mae: 1.5859 - val_loss: 1.8451 - val_mae: 1.8451\n",
      "Epoch 30/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5672 - mae: 1.5672 - val_loss: 1.7722 - val_mae: 1.7722\n",
      "Epoch 31/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5581 - mae: 1.5581 - val_loss: 1.7668 - val_mae: 1.7668\n",
      "Epoch 32/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5558 - mae: 1.5558 - val_loss: 1.7599 - val_mae: 1.7599\n",
      "Epoch 33/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5385 - mae: 1.5385 - val_loss: 1.7828 - val_mae: 1.7828\n",
      "Epoch 34/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5320 - mae: 1.5320 - val_loss: 1.7841 - val_mae: 1.7841\n",
      "Epoch 35/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5256 - mae: 1.5256 - val_loss: 1.9458 - val_mae: 1.9458\n",
      "Epoch 36/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5336 - mae: 1.5336 - val_loss: 1.7195 - val_mae: 1.7195\n",
      "Epoch 37/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5435 - mae: 1.5435 - val_loss: 1.7220 - val_mae: 1.7220\n",
      "Epoch 38/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5233 - mae: 1.5233 - val_loss: 1.7214 - val_mae: 1.7214\n",
      "Epoch 39/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5410 - mae: 1.5410 - val_loss: 1.7125 - val_mae: 1.7125\n",
      "Epoch 40/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5155 - mae: 1.5155 - val_loss: 1.7134 - val_mae: 1.7134\n",
      "Epoch 41/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5255 - mae: 1.5255 - val_loss: 1.7155 - val_mae: 1.7155\n",
      "Epoch 42/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5180 - mae: 1.5180 - val_loss: 1.8905 - val_mae: 1.8905\n",
      "Epoch 43/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5257 - mae: 1.5257 - val_loss: 1.7364 - val_mae: 1.7364\n",
      "Epoch 44/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5250 - mae: 1.5250 - val_loss: 1.7180 - val_mae: 1.7180\n",
      "Epoch 45/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5159 - mae: 1.5159 - val_loss: 1.8677 - val_mae: 1.8677\n",
      "Epoch 46/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5164 - mae: 1.5164 - val_loss: 1.7616 - val_mae: 1.7616\n",
      "Epoch 47/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5022 - mae: 1.5022 - val_loss: 1.7191 - val_mae: 1.7191\n",
      "Epoch 48/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5156 - mae: 1.5156 - val_loss: 1.7070 - val_mae: 1.7070\n",
      "Epoch 49/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5001 - mae: 1.5001 - val_loss: 1.6979 - val_mae: 1.6979\n",
      "Epoch 50/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5013 - mae: 1.5013 - val_loss: 1.7271 - val_mae: 1.7271\n",
      "Epoch 51/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5051 - mae: 1.5051 - val_loss: 1.6981 - val_mae: 1.6981\n",
      "Epoch 52/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5076 - mae: 1.5076 - val_loss: 1.7800 - val_mae: 1.7800\n",
      "Epoch 53/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5101 - mae: 1.5101 - val_loss: 1.7433 - val_mae: 1.7433\n",
      "Epoch 54/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5053 - mae: 1.5053 - val_loss: 1.7006 - val_mae: 1.7006\n",
      "Epoch 55/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5123 - mae: 1.5123 - val_loss: 1.7000 - val_mae: 1.7000\n",
      "Epoch 56/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4891 - mae: 1.4891 - val_loss: 1.6961 - val_mae: 1.6961\n",
      "Epoch 57/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5008 - mae: 1.5008 - val_loss: 1.7366 - val_mae: 1.7366\n",
      "Epoch 58/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5103 - mae: 1.5103 - val_loss: 1.7223 - val_mae: 1.7223\n",
      "Epoch 59/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4981 - mae: 1.4981 - val_loss: 1.7628 - val_mae: 1.7628\n",
      "Epoch 60/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4986 - mae: 1.4986 - val_loss: 1.6947 - val_mae: 1.6947\n",
      "Epoch 61/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4952 - mae: 1.4952 - val_loss: 1.7487 - val_mae: 1.7487\n",
      "Epoch 62/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5026 - mae: 1.5026 - val_loss: 1.7028 - val_mae: 1.7028\n",
      "Epoch 63/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4957 - mae: 1.4957 - val_loss: 1.7313 - val_mae: 1.7313\n",
      "Epoch 64/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.5028 - mae: 1.5028 - val_loss: 1.7060 - val_mae: 1.7060\n",
      "Epoch 65/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4877 - mae: 1.4877 - val_loss: 1.6978 - val_mae: 1.6978\n",
      "Epoch 66/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4928 - mae: 1.4928 - val_loss: 1.6887 - val_mae: 1.6887\n",
      "Epoch 67/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4936 - mae: 1.4936 - val_loss: 1.7048 - val_mae: 1.7048\n",
      "Epoch 68/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4935 - mae: 1.4935 - val_loss: 1.7205 - val_mae: 1.7205\n",
      "Epoch 69/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4972 - mae: 1.4972 - val_loss: 1.7088 - val_mae: 1.7088\n",
      "Epoch 70/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5001 - mae: 1.5001 - val_loss: 1.6809 - val_mae: 1.6809\n",
      "Epoch 71/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4910 - mae: 1.4910 - val_loss: 1.8162 - val_mae: 1.8162\n",
      "Epoch 72/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4915 - mae: 1.4915 - val_loss: 1.7410 - val_mae: 1.7410\n",
      "Epoch 73/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4814 - mae: 1.4814 - val_loss: 1.7068 - val_mae: 1.7068\n",
      "Epoch 74/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4783 - mae: 1.4783 - val_loss: 1.7358 - val_mae: 1.7358\n",
      "Epoch 75/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4879 - mae: 1.4879 - val_loss: 1.6841 - val_mae: 1.6841\n",
      "Epoch 76/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4953 - mae: 1.4953 - val_loss: 1.7052 - val_mae: 1.7052\n",
      "Epoch 77/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4761 - mae: 1.4761 - val_loss: 1.7005 - val_mae: 1.7005\n",
      "Epoch 78/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4855 - mae: 1.4855 - val_loss: 1.6823 - val_mae: 1.6823\n",
      "Epoch 79/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4734 - mae: 1.4734 - val_loss: 1.6748 - val_mae: 1.6748\n",
      "Epoch 80/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4912 - mae: 1.4912 - val_loss: 1.7635 - val_mae: 1.7635\n",
      "Epoch 81/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4741 - mae: 1.4741 - val_loss: 1.7002 - val_mae: 1.7002\n",
      "Epoch 82/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4776 - mae: 1.4776 - val_loss: 1.6867 - val_mae: 1.6867\n",
      "Epoch 83/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4766 - mae: 1.4766 - val_loss: 1.6913 - val_mae: 1.6913\n",
      "Epoch 84/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4928 - mae: 1.4928 - val_loss: 1.7512 - val_mae: 1.7512\n",
      "Epoch 85/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4805 - mae: 1.4805 - val_loss: 1.6738 - val_mae: 1.6738\n",
      "Epoch 86/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4675 - mae: 1.4675 - val_loss: 1.6870 - val_mae: 1.6870\n",
      "Epoch 87/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4742 - mae: 1.4742 - val_loss: 1.6795 - val_mae: 1.6795\n",
      "Epoch 88/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4710 - mae: 1.4710 - val_loss: 1.6650 - val_mae: 1.6650\n",
      "Epoch 89/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4595 - mae: 1.4595 - val_loss: 1.6696 - val_mae: 1.6696\n",
      "Epoch 90/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4707 - mae: 1.4707 - val_loss: 1.6787 - val_mae: 1.6787\n",
      "Epoch 91/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4852 - mae: 1.4852 - val_loss: 1.6581 - val_mae: 1.6581\n",
      "Epoch 92/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4602 - mae: 1.4602 - val_loss: 1.6865 - val_mae: 1.6865\n",
      "Epoch 93/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4681 - mae: 1.4681 - val_loss: 1.6521 - val_mae: 1.6521\n",
      "Epoch 94/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4564 - mae: 1.4564 - val_loss: 1.6775 - val_mae: 1.6775\n",
      "Epoch 95/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4600 - mae: 1.4600 - val_loss: 1.6404 - val_mae: 1.6404\n",
      "Epoch 96/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4538 - mae: 1.4538 - val_loss: 1.6436 - val_mae: 1.6436\n",
      "Epoch 97/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4472 - mae: 1.4472 - val_loss: 1.6797 - val_mae: 1.6797\n",
      "Epoch 98/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4424 - mae: 1.4424 - val_loss: 1.6384 - val_mae: 1.6384\n",
      "Epoch 99/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4320 - mae: 1.4320 - val_loss: 1.6686 - val_mae: 1.6686\n",
      "Epoch 100/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4379 - mae: 1.4379 - val_loss: 1.6270 - val_mae: 1.6270\n",
      "Epoch 101/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4423 - mae: 1.4423 - val_loss: 1.6400 - val_mae: 1.6400\n",
      "Epoch 102/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4358 - mae: 1.4358 - val_loss: 1.6080 - val_mae: 1.6080\n",
      "Epoch 103/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4305 - mae: 1.4305 - val_loss: 1.6330 - val_mae: 1.6330\n",
      "Epoch 104/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4341 - mae: 1.4341 - val_loss: 1.6414 - val_mae: 1.6414\n",
      "Epoch 105/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4184 - mae: 1.4184 - val_loss: 1.6255 - val_mae: 1.6255\n",
      "Epoch 106/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4360 - mae: 1.4360 - val_loss: 1.5949 - val_mae: 1.5949\n",
      "Epoch 107/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4377 - mae: 1.4377 - val_loss: 1.5964 - val_mae: 1.5964\n",
      "Epoch 108/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4184 - mae: 1.4184 - val_loss: 1.6008 - val_mae: 1.6008\n",
      "Epoch 109/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4163 - mae: 1.4163 - val_loss: 1.6101 - val_mae: 1.6101\n",
      "Epoch 110/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4317 - mae: 1.4317 - val_loss: 1.6230 - val_mae: 1.6230\n",
      "Epoch 111/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4318 - mae: 1.4318 - val_loss: 1.6232 - val_mae: 1.6232\n",
      "Epoch 112/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4173 - mae: 1.4173 - val_loss: 1.6035 - val_mae: 1.6035\n",
      "Epoch 113/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4246 - mae: 1.4246 - val_loss: 1.6027 - val_mae: 1.6027\n",
      "Epoch 114/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4305 - mae: 1.4305 - val_loss: 1.5919 - val_mae: 1.5919\n",
      "Epoch 115/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4208 - mae: 1.4208 - val_loss: 1.6021 - val_mae: 1.6021\n",
      "Epoch 116/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4352 - mae: 1.4352 - val_loss: 1.6004 - val_mae: 1.6004\n",
      "Epoch 117/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4273 - mae: 1.4273 - val_loss: 1.5879 - val_mae: 1.5879\n",
      "Epoch 118/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4206 - mae: 1.4206 - val_loss: 1.6441 - val_mae: 1.6441\n",
      "Epoch 119/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4205 - mae: 1.4205 - val_loss: 1.5902 - val_mae: 1.5902\n",
      "Epoch 120/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4254 - mae: 1.4254 - val_loss: 1.6092 - val_mae: 1.6092\n",
      "Epoch 121/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4272 - mae: 1.4272 - val_loss: 1.5943 - val_mae: 1.5943\n",
      "Epoch 122/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4129 - mae: 1.4129 - val_loss: 1.5875 - val_mae: 1.5875\n",
      "Epoch 123/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4166 - mae: 1.4166 - val_loss: 1.5810 - val_mae: 1.5810\n",
      "Epoch 124/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4149 - mae: 1.4149 - val_loss: 1.6844 - val_mae: 1.6844\n",
      "Epoch 125/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4302 - mae: 1.4302 - val_loss: 1.6343 - val_mae: 1.6343\n",
      "Epoch 126/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4128 - mae: 1.4128 - val_loss: 1.5920 - val_mae: 1.5920\n",
      "Epoch 127/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4001 - mae: 1.4001 - val_loss: 1.6028 - val_mae: 1.6028\n",
      "Epoch 128/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4150 - mae: 1.4150 - val_loss: 1.6624 - val_mae: 1.6624\n",
      "Epoch 129/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4018 - mae: 1.4018 - val_loss: 1.6151 - val_mae: 1.6151\n",
      "Epoch 130/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4161 - mae: 1.4161 - val_loss: 1.6187 - val_mae: 1.6187\n",
      "Epoch 131/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4151 - mae: 1.4151 - val_loss: 1.5975 - val_mae: 1.5975\n",
      "Epoch 132/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4137 - mae: 1.4137 - val_loss: 1.5699 - val_mae: 1.5699\n",
      "Epoch 133/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4062 - mae: 1.4062 - val_loss: 1.5992 - val_mae: 1.5992\n",
      "Epoch 134/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4053 - mae: 1.4053 - val_loss: 1.5842 - val_mae: 1.5842\n",
      "Epoch 135/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4127 - mae: 1.4127 - val_loss: 1.5804 - val_mae: 1.5804\n",
      "Epoch 136/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4084 - mae: 1.4084 - val_loss: 1.5870 - val_mae: 1.5870\n",
      "Epoch 137/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3983 - mae: 1.3983 - val_loss: 1.5806 - val_mae: 1.5806\n",
      "Epoch 138/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4089 - mae: 1.4089 - val_loss: 1.5651 - val_mae: 1.5651\n",
      "Epoch 139/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4096 - mae: 1.4096 - val_loss: 1.5712 - val_mae: 1.5712\n",
      "Epoch 140/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3923 - mae: 1.3923 - val_loss: 1.5908 - val_mae: 1.5908\n",
      "Epoch 141/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4172 - mae: 1.4172 - val_loss: 1.5818 - val_mae: 1.5818\n",
      "Epoch 142/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4126 - mae: 1.4126 - val_loss: 1.6341 - val_mae: 1.6341\n",
      "Epoch 143/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4102 - mae: 1.4102 - val_loss: 1.5799 - val_mae: 1.5799\n",
      "Epoch 144/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3933 - mae: 1.3933 - val_loss: 1.6123 - val_mae: 1.6123\n",
      "Epoch 145/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4032 - mae: 1.4032 - val_loss: 1.5708 - val_mae: 1.5708\n",
      "Epoch 146/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4041 - mae: 1.4041 - val_loss: 1.5973 - val_mae: 1.5973\n",
      "Epoch 147/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3958 - mae: 1.3958 - val_loss: 1.5754 - val_mae: 1.5754\n",
      "Epoch 148/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4042 - mae: 1.4042 - val_loss: 1.5777 - val_mae: 1.5777\n",
      "Epoch 149/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3977 - mae: 1.3977 - val_loss: 1.5694 - val_mae: 1.5694\n",
      "Epoch 150/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3968 - mae: 1.3968 - val_loss: 1.5650 - val_mae: 1.5650\n",
      "Epoch 151/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4026 - mae: 1.4026 - val_loss: 1.6342 - val_mae: 1.6342\n",
      "Epoch 152/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4091 - mae: 1.4091 - val_loss: 1.5779 - val_mae: 1.5779\n",
      "Epoch 153/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3990 - mae: 1.3990 - val_loss: 1.6134 - val_mae: 1.6134\n",
      "Epoch 154/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3984 - mae: 1.3984 - val_loss: 1.5800 - val_mae: 1.5800\n",
      "Epoch 155/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4100 - mae: 1.4100 - val_loss: 1.5765 - val_mae: 1.5765\n",
      "Epoch 156/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3890 - mae: 1.3890 - val_loss: 1.5546 - val_mae: 1.5546\n",
      "Epoch 157/1000\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8944 - mae: 0.894 - 0s 2ms/step - loss: 1.4005 - mae: 1.4005 - val_loss: 1.5627 - val_mae: 1.5627\n",
      "Epoch 158/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3880 - mae: 1.3880 - val_loss: 1.5751 - val_mae: 1.5751\n",
      "Epoch 159/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4033 - mae: 1.4033 - val_loss: 1.5692 - val_mae: 1.5692\n",
      "Epoch 160/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3854 - mae: 1.3854 - val_loss: 1.5646 - val_mae: 1.5646\n",
      "Epoch 161/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3836 - mae: 1.3836 - val_loss: 1.5936 - val_mae: 1.5936\n",
      "Epoch 162/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4034 - mae: 1.4034 - val_loss: 1.5591 - val_mae: 1.5591\n",
      "Epoch 163/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3829 - mae: 1.3829 - val_loss: 1.5544 - val_mae: 1.5544\n",
      "Epoch 164/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4020 - mae: 1.4020 - val_loss: 1.5825 - val_mae: 1.5825\n",
      "Epoch 165/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3821 - mae: 1.3821 - val_loss: 1.5684 - val_mae: 1.5684\n",
      "Epoch 166/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3880 - mae: 1.3880 - val_loss: 1.5528 - val_mae: 1.5528\n",
      "Epoch 167/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3950 - mae: 1.3950 - val_loss: 1.5785 - val_mae: 1.5785\n",
      "Epoch 168/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4035 - mae: 1.4035 - val_loss: 1.6089 - val_mae: 1.6089\n",
      "Epoch 169/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3939 - mae: 1.3939 - val_loss: 1.5611 - val_mae: 1.5611\n",
      "Epoch 170/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3910 - mae: 1.3910 - val_loss: 1.5551 - val_mae: 1.5551\n",
      "Epoch 171/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3934 - mae: 1.3934 - val_loss: 1.5845 - val_mae: 1.5845\n",
      "Epoch 172/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3900 - mae: 1.3900 - val_loss: 1.5521 - val_mae: 1.5521\n",
      "Epoch 173/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3876 - mae: 1.3876 - val_loss: 1.5486 - val_mae: 1.5486\n",
      "Epoch 174/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3928 - mae: 1.3928 - val_loss: 1.5883 - val_mae: 1.5883\n",
      "Epoch 175/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3872 - mae: 1.3872 - val_loss: 1.5587 - val_mae: 1.5587\n",
      "Epoch 176/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3818 - mae: 1.3818 - val_loss: 1.5752 - val_mae: 1.5752\n",
      "Epoch 177/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3983 - mae: 1.3983 - val_loss: 1.5533 - val_mae: 1.5533\n",
      "Epoch 178/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3871 - mae: 1.3871 - val_loss: 1.5775 - val_mae: 1.5775\n",
      "Epoch 179/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3806 - mae: 1.3806 - val_loss: 1.5685 - val_mae: 1.5685\n",
      "Epoch 180/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3912 - mae: 1.3912 - val_loss: 1.6085 - val_mae: 1.6085\n",
      "Epoch 181/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3790 - mae: 1.3790 - val_loss: 1.7182 - val_mae: 1.7182\n",
      "Epoch 182/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4034 - mae: 1.4034 - val_loss: 1.5659 - val_mae: 1.5659\n",
      "Epoch 183/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3955 - mae: 1.3955 - val_loss: 1.5562 - val_mae: 1.5562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3718 - mae: 1.3718 - val_loss: 1.5517 - val_mae: 1.5517\n",
      "Epoch 185/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3771 - mae: 1.3771 - val_loss: 1.5514 - val_mae: 1.5514\n",
      "Epoch 186/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3847 - mae: 1.3847 - val_loss: 1.5871 - val_mae: 1.5871\n",
      "Epoch 187/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3886 - mae: 1.3886 - val_loss: 1.5456 - val_mae: 1.5456\n",
      "Epoch 188/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3802 - mae: 1.3802 - val_loss: 1.5602 - val_mae: 1.5602\n",
      "Epoch 189/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3820 - mae: 1.3820 - val_loss: 1.5699 - val_mae: 1.5699\n",
      "Epoch 190/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3919 - mae: 1.3919 - val_loss: 1.5692 - val_mae: 1.5692\n",
      "Epoch 191/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3962 - mae: 1.3962 - val_loss: 1.5727 - val_mae: 1.5727\n",
      "Epoch 192/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3847 - mae: 1.3847 - val_loss: 1.5600 - val_mae: 1.5600\n",
      "Epoch 193/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3794 - mae: 1.3794 - val_loss: 1.5620 - val_mae: 1.5620\n",
      "Epoch 194/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3738 - mae: 1.3738 - val_loss: 1.5533 - val_mae: 1.5533\n",
      "Epoch 195/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3836 - mae: 1.3836 - val_loss: 1.6020 - val_mae: 1.6020\n",
      "Epoch 196/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3948 - mae: 1.3948 - val_loss: 1.5661 - val_mae: 1.5661\n",
      "Epoch 197/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3821 - mae: 1.3821 - val_loss: 1.6102 - val_mae: 1.6102\n",
      "Epoch 198/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3802 - mae: 1.3802 - val_loss: 1.5543 - val_mae: 1.5543\n",
      "Epoch 199/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3815 - mae: 1.3815 - val_loss: 1.5528 - val_mae: 1.5528\n",
      "Epoch 200/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3805 - mae: 1.3805 - val_loss: 1.5620 - val_mae: 1.5620\n",
      "Epoch 201/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3808 - mae: 1.3808 - val_loss: 1.5506 - val_mae: 1.5506\n",
      "Epoch 202/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3801 - mae: 1.3801 - val_loss: 1.5418 - val_mae: 1.5418\n",
      "Epoch 203/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3935 - mae: 1.3935 - val_loss: 1.5567 - val_mae: 1.5567\n",
      "Epoch 204/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3870 - mae: 1.3870 - val_loss: 1.5441 - val_mae: 1.5441\n",
      "Epoch 205/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3791 - mae: 1.3791 - val_loss: 1.6550 - val_mae: 1.6550\n",
      "Epoch 206/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3939 - mae: 1.3939 - val_loss: 1.5620 - val_mae: 1.5620\n",
      "Epoch 207/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3800 - mae: 1.3800 - val_loss: 1.5468 - val_mae: 1.5468\n",
      "Epoch 208/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3878 - mae: 1.3878 - val_loss: 1.5885 - val_mae: 1.5885\n",
      "Epoch 209/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3846 - mae: 1.3846 - val_loss: 1.5536 - val_mae: 1.5536\n",
      "Epoch 210/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3789 - mae: 1.3789 - val_loss: 1.5568 - val_mae: 1.5568\n",
      "Epoch 211/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3790 - mae: 1.3790 - val_loss: 1.5449 - val_mae: 1.5449\n",
      "Epoch 212/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3758 - mae: 1.3758 - val_loss: 1.5655 - val_mae: 1.5655\n",
      "Epoch 213/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3637 - mae: 1.3637 - val_loss: 1.6057 - val_mae: 1.6057\n",
      "Epoch 214/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3817 - mae: 1.3817 - val_loss: 1.6241 - val_mae: 1.6241\n",
      "Epoch 215/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3825 - mae: 1.3825 - val_loss: 1.5425 - val_mae: 1.5425\n",
      "Epoch 216/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3688 - mae: 1.3688 - val_loss: 1.5496 - val_mae: 1.5496\n",
      "Epoch 217/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3634 - mae: 1.3634 - val_loss: 1.5685 - val_mae: 1.5685\n",
      "Epoch 218/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3801 - mae: 1.3801 - val_loss: 1.5471 - val_mae: 1.5471\n",
      "Epoch 219/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3796 - mae: 1.3796 - val_loss: 1.5879 - val_mae: 1.5879\n",
      "Epoch 220/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3722 - mae: 1.3722 - val_loss: 1.5935 - val_mae: 1.5935\n",
      "Epoch 221/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3827 - mae: 1.3827 - val_loss: 1.5482 - val_mae: 1.5482\n",
      "Epoch 222/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3732 - mae: 1.3732 - val_loss: 1.5462 - val_mae: 1.5462\n",
      "Epoch 223/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3738 - mae: 1.3738 - val_loss: 1.6042 - val_mae: 1.6042\n",
      "Epoch 224/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3679 - mae: 1.3679 - val_loss: 1.5600 - val_mae: 1.5600\n",
      "Epoch 225/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3720 - mae: 1.3720 - val_loss: 1.5706 - val_mae: 1.5706\n",
      "Epoch 226/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3867 - mae: 1.3867 - val_loss: 1.5417 - val_mae: 1.5417\n",
      "Epoch 227/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3728 - mae: 1.3728 - val_loss: 1.5694 - val_mae: 1.5694\n",
      "Epoch 228/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3755 - mae: 1.3755 - val_loss: 1.5744 - val_mae: 1.5744\n",
      "Epoch 229/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3815 - mae: 1.3815 - val_loss: 1.5505 - val_mae: 1.5505\n",
      "Epoch 230/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3757 - mae: 1.3757 - val_loss: 1.5640 - val_mae: 1.5640\n",
      "Epoch 231/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3734 - mae: 1.3734 - val_loss: 1.5373 - val_mae: 1.5373\n",
      "Epoch 232/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3796 - mae: 1.3796 - val_loss: 1.5623 - val_mae: 1.5623\n",
      "Epoch 233/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3711 - mae: 1.3711 - val_loss: 1.5666 - val_mae: 1.5666\n",
      "Epoch 234/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3676 - mae: 1.3676 - val_loss: 1.5453 - val_mae: 1.5453\n",
      "Epoch 235/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3652 - mae: 1.3652 - val_loss: 1.5486 - val_mae: 1.5486\n",
      "Epoch 236/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3753 - mae: 1.3753 - val_loss: 1.5555 - val_mae: 1.5555\n",
      "Epoch 237/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3691 - mae: 1.3691 - val_loss: 1.5448 - val_mae: 1.5448\n",
      "Epoch 238/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3727 - mae: 1.3727 - val_loss: 1.5715 - val_mae: 1.5715\n",
      "Epoch 239/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3769 - mae: 1.3769 - val_loss: 1.5454 - val_mae: 1.5454\n",
      "Epoch 240/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3743 - mae: 1.3743 - val_loss: 1.5650 - val_mae: 1.5650\n",
      "Epoch 241/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3740 - mae: 1.3740 - val_loss: 1.5626 - val_mae: 1.5626\n",
      "Epoch 242/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3754 - mae: 1.3754 - val_loss: 1.5385 - val_mae: 1.5385\n",
      "Epoch 243/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3682 - mae: 1.3682 - val_loss: 1.5477 - val_mae: 1.5477\n",
      "Epoch 244/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3686 - mae: 1.3686 - val_loss: 1.5402 - val_mae: 1.5402\n",
      "Epoch 245/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3736 - mae: 1.3736 - val_loss: 1.5508 - val_mae: 1.5508\n",
      "Epoch 246/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3722 - mae: 1.3722 - val_loss: 1.5938 - val_mae: 1.5938\n",
      "Epoch 247/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3684 - mae: 1.3684 - val_loss: 1.5494 - val_mae: 1.5494\n",
      "Epoch 248/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3737 - mae: 1.3737 - val_loss: 1.5683 - val_mae: 1.5683\n",
      "Epoch 249/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3637 - mae: 1.3637 - val_loss: 1.5590 - val_mae: 1.5590\n",
      "Epoch 250/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3669 - mae: 1.3669 - val_loss: 1.5810 - val_mae: 1.5810\n",
      "Epoch 251/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3775 - mae: 1.3775 - val_loss: 1.5443 - val_mae: 1.5443\n",
      "Epoch 252/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3686 - mae: 1.3686 - val_loss: 1.5692 - val_mae: 1.5692\n",
      "Epoch 253/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3699 - mae: 1.3699 - val_loss: 1.5969 - val_mae: 1.5969\n",
      "Epoch 254/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3792 - mae: 1.3792 - val_loss: 1.5557 - val_mae: 1.5557\n",
      "Epoch 255/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3722 - mae: 1.3722 - val_loss: 1.5370 - val_mae: 1.5370\n",
      "Epoch 256/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3652 - mae: 1.3652 - val_loss: 1.5478 - val_mae: 1.5478\n",
      "Epoch 257/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3638 - mae: 1.3638 - val_loss: 1.5510 - val_mae: 1.5510\n",
      "Epoch 258/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3635 - mae: 1.3635 - val_loss: 1.5404 - val_mae: 1.5404\n",
      "Epoch 259/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3751 - mae: 1.3751 - val_loss: 1.5738 - val_mae: 1.5738\n",
      "Epoch 260/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3647 - mae: 1.3647 - val_loss: 1.6170 - val_mae: 1.6170\n",
      "Epoch 261/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3757 - mae: 1.3757 - val_loss: 1.5559 - val_mae: 1.5559\n",
      "Epoch 262/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3631 - mae: 1.3631 - val_loss: 1.5690 - val_mae: 1.5690\n",
      "Epoch 263/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3699 - mae: 1.3699 - val_loss: 1.5563 - val_mae: 1.5563\n",
      "Epoch 264/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3606 - mae: 1.3606 - val_loss: 1.5707 - val_mae: 1.5707\n",
      "Epoch 265/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3683 - mae: 1.3683 - val_loss: 1.5424 - val_mae: 1.5424\n",
      "Epoch 266/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3663 - mae: 1.3663 - val_loss: 1.5394 - val_mae: 1.5394\n",
      "Epoch 267/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3631 - mae: 1.3631 - val_loss: 1.5803 - val_mae: 1.5803\n",
      "Epoch 268/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3736 - mae: 1.3736 - val_loss: 1.5513 - val_mae: 1.5513\n",
      "Epoch 269/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3672 - mae: 1.3672 - val_loss: 1.5509 - val_mae: 1.5509\n",
      "Epoch 270/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3702 - mae: 1.3702 - val_loss: 1.5403 - val_mae: 1.5403\n",
      "Epoch 271/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3685 - mae: 1.3685 - val_loss: 1.5820 - val_mae: 1.5820\n",
      "Epoch 272/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3704 - mae: 1.3704 - val_loss: 1.5880 - val_mae: 1.5880\n",
      "Epoch 273/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3674 - mae: 1.3674 - val_loss: 1.5525 - val_mae: 1.5525\n",
      "Epoch 274/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3739 - mae: 1.3739 - val_loss: 1.5544 - val_mae: 1.5544\n",
      "Epoch 275/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3586 - mae: 1.3586 - val_loss: 1.5515 - val_mae: 1.5515\n",
      "Epoch 276/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3678 - mae: 1.3678 - val_loss: 1.5408 - val_mae: 1.5408\n",
      "Epoch 277/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3636 - mae: 1.3636 - val_loss: 1.5469 - val_mae: 1.5469\n",
      "Epoch 278/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3679 - mae: 1.3679 - val_loss: 1.5607 - val_mae: 1.5607\n",
      "Epoch 279/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3626 - mae: 1.3626 - val_loss: 1.5363 - val_mae: 1.5363\n",
      "Epoch 280/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3667 - mae: 1.3667 - val_loss: 1.5411 - val_mae: 1.5411\n",
      "Epoch 281/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3612 - mae: 1.3612 - val_loss: 1.5576 - val_mae: 1.5576\n",
      "Epoch 282/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3622 - mae: 1.3622 - val_loss: 1.5557 - val_mae: 1.5557\n",
      "Epoch 283/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3708 - mae: 1.3708 - val_loss: 1.5569 - val_mae: 1.5569\n",
      "Epoch 284/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3735 - mae: 1.3735 - val_loss: 1.5471 - val_mae: 1.5471\n",
      "Epoch 285/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3535 - mae: 1.3535 - val_loss: 1.5683 - val_mae: 1.5683\n",
      "Epoch 286/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3748 - mae: 1.3748 - val_loss: 1.5825 - val_mae: 1.5825\n",
      "Epoch 287/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3691 - mae: 1.3691 - val_loss: 1.5529 - val_mae: 1.5529\n",
      "Epoch 288/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3667 - mae: 1.3667 - val_loss: 1.5466 - val_mae: 1.5466\n",
      "Epoch 289/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3593 - mae: 1.3593 - val_loss: 1.5587 - val_mae: 1.5587\n",
      "Epoch 290/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3658 - mae: 1.3658 - val_loss: 1.5493 - val_mae: 1.5493\n",
      "Epoch 291/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3688 - mae: 1.3688 - val_loss: 1.5587 - val_mae: 1.5587\n",
      "Epoch 292/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3603 - mae: 1.3603 - val_loss: 1.5650 - val_mae: 1.5650\n",
      "Epoch 293/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3694 - mae: 1.3694 - val_loss: 1.5459 - val_mae: 1.5459\n",
      "Epoch 294/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3531 - mae: 1.3531 - val_loss: 1.5669 - val_mae: 1.5669\n",
      "Epoch 295/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3603 - mae: 1.3603 - val_loss: 1.5426 - val_mae: 1.5426\n",
      "Epoch 296/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3624 - mae: 1.3624 - val_loss: 1.5387 - val_mae: 1.5387\n",
      "Epoch 297/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3600 - mae: 1.3600 - val_loss: 1.6029 - val_mae: 1.6029\n",
      "Epoch 298/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3829 - mae: 1.3829 - val_loss: 1.5658 - val_mae: 1.5658\n",
      "Epoch 299/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3606 - mae: 1.3606 - val_loss: 1.5349 - val_mae: 1.5349\n",
      "Epoch 300/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3638 - mae: 1.3638 - val_loss: 1.5387 - val_mae: 1.5387\n",
      "Epoch 301/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3686 - mae: 1.3686 - val_loss: 1.5347 - val_mae: 1.5347\n",
      "Epoch 302/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3516 - mae: 1.3516 - val_loss: 1.5468 - val_mae: 1.5468\n",
      "Epoch 303/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3601 - mae: 1.3601 - val_loss: 1.5475 - val_mae: 1.5475\n",
      "Epoch 304/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3599 - mae: 1.3599 - val_loss: 1.5373 - val_mae: 1.5373\n",
      "Epoch 305/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3558 - mae: 1.3558 - val_loss: 1.5759 - val_mae: 1.5759\n",
      "Epoch 306/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3664 - mae: 1.3664 - val_loss: 1.5574 - val_mae: 1.5574\n",
      "Epoch 307/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3587 - mae: 1.3587 - val_loss: 1.5624 - val_mae: 1.5624\n",
      "Epoch 308/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3720 - mae: 1.3720 - val_loss: 1.5422 - val_mae: 1.5422\n",
      "Epoch 309/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3655 - mae: 1.3655 - val_loss: 1.5557 - val_mae: 1.5557\n",
      "Epoch 310/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3700 - mae: 1.3700 - val_loss: 1.5387 - val_mae: 1.5387\n",
      "Epoch 311/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3553 - mae: 1.3553 - val_loss: 1.5389 - val_mae: 1.5389\n",
      "Epoch 312/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3540 - mae: 1.3540 - val_loss: 1.5397 - val_mae: 1.5397\n",
      "Epoch 313/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3545 - mae: 1.3545 - val_loss: 1.5731 - val_mae: 1.5731\n",
      "Epoch 314/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3633 - mae: 1.3633 - val_loss: 1.5365 - val_mae: 1.5365\n",
      "Epoch 315/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3561 - mae: 1.3561 - val_loss: 1.5551 - val_mae: 1.5551\n",
      "Epoch 316/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3585 - mae: 1.3585 - val_loss: 1.5451 - val_mae: 1.5451\n",
      "Epoch 317/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3535 - mae: 1.3535 - val_loss: 1.5439 - val_mae: 1.5439\n",
      "Epoch 318/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3549 - mae: 1.3549 - val_loss: 1.5563 - val_mae: 1.5563\n",
      "Epoch 319/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3657 - mae: 1.3657 - val_loss: 1.5716 - val_mae: 1.5716\n",
      "Epoch 320/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3675 - mae: 1.3675 - val_loss: 1.5391 - val_mae: 1.5391\n",
      "Epoch 321/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3562 - mae: 1.3562 - val_loss: 1.5526 - val_mae: 1.5526\n",
      "Epoch 322/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3600 - mae: 1.3600 - val_loss: 1.5650 - val_mae: 1.5650\n",
      "Epoch 323/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3613 - mae: 1.3613 - val_loss: 1.5442 - val_mae: 1.5442\n",
      "Epoch 324/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3471 - mae: 1.3471 - val_loss: 1.5415 - val_mae: 1.5415\n",
      "Epoch 325/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3560 - mae: 1.3560 - val_loss: 1.5568 - val_mae: 1.5568\n",
      "Epoch 326/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3635 - mae: 1.3635 - val_loss: 1.5543 - val_mae: 1.5543\n",
      "Epoch 327/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3520 - mae: 1.3520 - val_loss: 1.5763 - val_mae: 1.5763\n",
      "Epoch 328/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3623 - mae: 1.3623 - val_loss: 1.5465 - val_mae: 1.5465\n",
      "Epoch 329/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3565 - mae: 1.3565 - val_loss: 1.5941 - val_mae: 1.5941\n",
      "Epoch 330/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3555 - mae: 1.3555 - val_loss: 1.5581 - val_mae: 1.5581\n",
      "Epoch 331/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3603 - mae: 1.3603 - val_loss: 1.5479 - val_mae: 1.5479\n",
      "Epoch 332/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3635 - mae: 1.3635 - val_loss: 1.6506 - val_mae: 1.6506\n",
      "Epoch 333/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3805 - mae: 1.3805 - val_loss: 1.5473 - val_mae: 1.5473\n",
      "Epoch 334/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3570 - mae: 1.3570 - val_loss: 1.5548 - val_mae: 1.5548\n",
      "Epoch 335/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3523 - mae: 1.3523 - val_loss: 1.5552 - val_mae: 1.5552\n",
      "Epoch 336/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3599 - mae: 1.3599 - val_loss: 1.5353 - val_mae: 1.5353\n",
      "Epoch 337/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3510 - mae: 1.3510 - val_loss: 1.5513 - val_mae: 1.5513\n",
      "Epoch 338/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3551 - mae: 1.3551 - val_loss: 1.6423 - val_mae: 1.6423\n",
      "Epoch 339/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3650 - mae: 1.3650 - val_loss: 1.6055 - val_mae: 1.6055\n",
      "Epoch 340/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3616 - mae: 1.3616 - val_loss: 1.5397 - val_mae: 1.5397\n",
      "Epoch 341/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3485 - mae: 1.3485 - val_loss: 1.5843 - val_mae: 1.5843\n",
      "Epoch 342/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3612 - mae: 1.3612 - val_loss: 1.5445 - val_mae: 1.5445\n",
      "Epoch 343/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3484 - mae: 1.3484 - val_loss: 1.5674 - val_mae: 1.5674\n",
      "Epoch 344/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3666 - mae: 1.3666 - val_loss: 1.5418 - val_mae: 1.5418\n",
      "Epoch 345/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3590 - mae: 1.3590 - val_loss: 1.5381 - val_mae: 1.5381\n",
      "Epoch 346/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3556 - mae: 1.3556 - val_loss: 1.5524 - val_mae: 1.5524\n",
      "Epoch 347/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3592 - mae: 1.3592 - val_loss: 1.5416 - val_mae: 1.5416\n",
      "Epoch 348/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3535 - mae: 1.3535 - val_loss: 1.5357 - val_mae: 1.5357\n",
      "Epoch 349/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3495 - mae: 1.3495 - val_loss: 1.6195 - val_mae: 1.6195\n",
      "Epoch 350/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3746 - mae: 1.3746 - val_loss: 1.5418 - val_mae: 1.5418\n",
      "Epoch 351/1000\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3486 - mae: 1.3486 - val_loss: 1.5516 - val_mae: 1.5516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x265871d2940>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_X, train_y, validation_split=0.3, epochs=1000, batch_size=32, verbose=1, callbacks=[es, cp, rlrp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a65164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction = model.predict(test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "62773933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_prediction.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b19c2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['Target'] = Y_prediction.flatten()\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc66333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31e471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ed1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9de5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9099686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56190b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3db3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb310f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1aa574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55143fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca36cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bd802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f8d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac12b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b797393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b11e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13a94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0da530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b19b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13fc955b",
   "metadata": {},
   "source": [
    "# 실험실"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a4089",
   "metadata": {},
   "source": [
    "## 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee409d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67601ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# StandardScaler 로 데이터 셋 변환 .fit( ) 과 .transform( ) 호출\n",
    "scaler.fit(train_X)\n",
    "train_X_scaled = scaler.transform(train_X)\n",
    "\n",
    "# transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "train_X_scaled = pd.DataFrame(data=train_X_scaled, columns=train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3099af85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscra Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Gender_F</th>\n",
       "      <th>Gender_I</th>\n",
       "      <th>Gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683382</td>\n",
       "      <td>0.632069</td>\n",
       "      <td>-0.629084</td>\n",
       "      <td>0.565272</td>\n",
       "      <td>0.150018</td>\n",
       "      <td>0.983043</td>\n",
       "      <td>0.490632</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>-0.689822</td>\n",
       "      <td>1.292920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.772731</td>\n",
       "      <td>-0.922116</td>\n",
       "      <td>-1.141679</td>\n",
       "      <td>-0.905870</td>\n",
       "      <td>-0.818704</td>\n",
       "      <td>-0.902715</td>\n",
       "      <td>-0.951302</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>1.449650</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.475366</td>\n",
       "      <td>0.832609</td>\n",
       "      <td>1.421295</td>\n",
       "      <td>0.970036</td>\n",
       "      <td>0.764655</td>\n",
       "      <td>0.652365</td>\n",
       "      <td>1.192304</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>1.449650</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100937</td>\n",
       "      <td>-0.019686</td>\n",
       "      <td>0.908700</td>\n",
       "      <td>0.878090</td>\n",
       "      <td>0.842598</td>\n",
       "      <td>1.300315</td>\n",
       "      <td>0.680084</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>-0.689822</td>\n",
       "      <td>1.292920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.771209</td>\n",
       "      <td>-1.724275</td>\n",
       "      <td>-1.269828</td>\n",
       "      <td>-1.407577</td>\n",
       "      <td>-1.384349</td>\n",
       "      <td>-1.340640</td>\n",
       "      <td>-1.403880</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>1.449650</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>-2.769687</td>\n",
       "      <td>-2.626705</td>\n",
       "      <td>-2.551316</td>\n",
       "      <td>-1.585474</td>\n",
       "      <td>-1.524646</td>\n",
       "      <td>-1.559602</td>\n",
       "      <td>-1.579298</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>1.449650</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>-1.063954</td>\n",
       "      <td>-0.972251</td>\n",
       "      <td>-1.397977</td>\n",
       "      <td>-1.027799</td>\n",
       "      <td>-0.916690</td>\n",
       "      <td>-1.166364</td>\n",
       "      <td>-1.028486</td>\n",
       "      <td>-0.659758</td>\n",
       "      <td>1.449650</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>0.017730</td>\n",
       "      <td>0.030449</td>\n",
       "      <td>-0.629084</td>\n",
       "      <td>-0.113332</td>\n",
       "      <td>0.254684</td>\n",
       "      <td>-0.160924</td>\n",
       "      <td>-0.421540</td>\n",
       "      <td>1.515707</td>\n",
       "      <td>-0.689822</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>-0.647921</td>\n",
       "      <td>-0.721576</td>\n",
       "      <td>-0.757233</td>\n",
       "      <td>-0.790937</td>\n",
       "      <td>-0.696222</td>\n",
       "      <td>-0.639066</td>\n",
       "      <td>-0.846052</td>\n",
       "      <td>1.515707</td>\n",
       "      <td>-0.689822</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>1.889876</td>\n",
       "      <td>1.434229</td>\n",
       "      <td>1.421295</td>\n",
       "      <td>2.001434</td>\n",
       "      <td>2.098596</td>\n",
       "      <td>1.653336</td>\n",
       "      <td>1.402805</td>\n",
       "      <td>1.515707</td>\n",
       "      <td>-0.689822</td>\n",
       "      <td>-0.773443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lenght  Diameter    Height  Whole Weight  Shucked Weight  \\\n",
       "0     0.683382  0.632069 -0.629084      0.565272        0.150018   \n",
       "1    -0.772731 -0.922116 -1.141679     -0.905870       -0.818704   \n",
       "2     0.475366  0.832609  1.421295      0.970036        0.764655   \n",
       "3     0.100937 -0.019686  0.908700      0.878090        0.842598   \n",
       "4    -1.771209 -1.724275 -1.269828     -1.407577       -1.384349   \n",
       "...        ...       ...       ...           ...             ...   \n",
       "1248 -2.769687 -2.626705 -2.551316     -1.585474       -1.524646   \n",
       "1249 -1.063954 -0.972251 -1.397977     -1.027799       -0.916690   \n",
       "1250  0.017730  0.030449 -0.629084     -0.113332        0.254684   \n",
       "1251 -0.647921 -0.721576 -0.757233     -0.790937       -0.696222   \n",
       "1252  1.889876  1.434229  1.421295      2.001434        2.098596   \n",
       "\n",
       "      Viscra Weight  Shell Weight  Gender_F  Gender_I  Gender_M  \n",
       "0          0.983043      0.490632 -0.659758 -0.689822  1.292920  \n",
       "1         -0.902715     -0.951302 -0.659758  1.449650 -0.773443  \n",
       "2          0.652365      1.192304 -0.659758  1.449650 -0.773443  \n",
       "3          1.300315      0.680084 -0.659758 -0.689822  1.292920  \n",
       "4         -1.340640     -1.403880 -0.659758  1.449650 -0.773443  \n",
       "...             ...           ...       ...       ...       ...  \n",
       "1248      -1.559602     -1.579298 -0.659758  1.449650 -0.773443  \n",
       "1249      -1.166364     -1.028486 -0.659758  1.449650 -0.773443  \n",
       "1250      -0.160924     -0.421540  1.515707 -0.689822 -0.773443  \n",
       "1251      -0.639066     -0.846052  1.515707 -0.689822 -0.773443  \n",
       "1252       1.653336      1.402805  1.515707 -0.689822 -0.773443  \n",
       "\n",
       "[1253 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b777935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f88d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cb07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394f5fec",
   "metadata": {},
   "source": [
    "# 딥러닝 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8b1171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bec1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 데이터 분리\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_X, train_y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be8da897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscra Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Gender_F</th>\n",
       "      <th>Gender_I</th>\n",
       "      <th>Gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.620</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.0865</td>\n",
       "      <td>0.5110</td>\n",
       "      <td>0.2715</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.2465</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>0.615</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.4030</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>0.2925</td>\n",
       "      <td>0.3650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.5580</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>0.425</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.2075</td>\n",
       "      <td>0.5105</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.565</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>0.4345</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>0.315</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>0.650</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.185</td>\n",
       "      <td>1.3750</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lenght  Diameter  Height  Whole Weight  Shucked Weight  Viscra Weight  \\\n",
       "525    0.620     0.470   0.145        1.0865          0.5110         0.2715   \n",
       "229    0.375     0.275   0.095        0.2465          0.1100         0.0415   \n",
       "690    0.615     0.505   0.190        1.4030          0.6715         0.2925   \n",
       "303    0.660     0.520   0.190        1.5580          0.7550         0.2980   \n",
       "289    0.600     0.465   0.165        1.0475          0.4650         0.2345   \n",
       "...      ...       ...     ...           ...             ...            ...   \n",
       "1238   0.425     0.340   0.120        0.3880          0.1490         0.0870   \n",
       "1147   0.650     0.505   0.175        1.2075          0.5105         0.2620   \n",
       "106    0.565     0.445   0.145        0.9255          0.4345         0.2120   \n",
       "1041   0.315     0.210   0.060        0.1250          0.0600         0.0375   \n",
       "1122   0.650     0.510   0.185        1.3750          0.5310         0.3840   \n",
       "\n",
       "      Shell Weight  Gender_F  Gender_I  Gender_M  \n",
       "525         0.2565         0         0         1  \n",
       "229         0.0775         0         1         0  \n",
       "690         0.3650         0         0         1  \n",
       "303         0.4000         0         0         1  \n",
       "289         0.3150         0         0         1  \n",
       "...            ...       ...       ...       ...  \n",
       "1238        0.1250         0         0         1  \n",
       "1147        0.3900         1         0         0  \n",
       "106         0.2475         0         0         1  \n",
       "1041        0.0350         0         1         0  \n",
       "1122        0.3985         1         0         0  \n",
       "\n",
       "[1002 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0114b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 선언\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=10, activation='elu'))\n",
    "model.add(Dense(32, activation='elu'))    \n",
    "model.add(Dense(64, activation='elu'))            \n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='adam', \n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "770c89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "# 모델 업데이트 및 저장\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_mae', verbose=0, save_best_only=True, mode = 'min')\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "early_stopping_callback = EarlyStopping(monitor='val_mae', patience=50, mode='min')\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=20, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "55f4958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.3302 - mae: 8.3302 - val_loss: 6.2271 - val_mae: 6.2271\n",
      "Epoch 2/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 3.6819 - mae: 3.6819 - val_loss: 1.8998 - val_mae: 1.8998\n",
      "Epoch 3/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.9440 - mae: 1.9440 - val_loss: 1.7800 - val_mae: 1.7800\n",
      "Epoch 4/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.8230 - mae: 1.8230 - val_loss: 1.7451 - val_mae: 1.7451\n",
      "Epoch 5/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7987 - mae: 1.7987 - val_loss: 1.7423 - val_mae: 1.7423\n",
      "Epoch 6/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.8050 - mae: 1.8050 - val_loss: 1.7490 - val_mae: 1.7490\n",
      "Epoch 7/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7970 - mae: 1.7970 - val_loss: 1.7376 - val_mae: 1.7376\n",
      "Epoch 8/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7891 - mae: 1.7891 - val_loss: 1.7495 - val_mae: 1.7495\n",
      "Epoch 9/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7953 - mae: 1.7953 - val_loss: 1.7420 - val_mae: 1.7420\n",
      "Epoch 10/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7887 - mae: 1.7887 - val_loss: 1.7382 - val_mae: 1.7382\n",
      "Epoch 11/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7784 - mae: 1.7784 - val_loss: 1.7313 - val_mae: 1.7313\n",
      "Epoch 12/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7772 - mae: 1.7772 - val_loss: 1.7462 - val_mae: 1.7462\n",
      "Epoch 13/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7890 - mae: 1.7890 - val_loss: 1.7247 - val_mae: 1.7247\n",
      "Epoch 14/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7848 - mae: 1.7848 - val_loss: 1.7390 - val_mae: 1.7390\n",
      "Epoch 15/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7938 - mae: 1.7938 - val_loss: 1.7260 - val_mae: 1.7260\n",
      "Epoch 16/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7687 - mae: 1.7687 - val_loss: 1.7454 - val_mae: 1.7454\n",
      "Epoch 17/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7857 - mae: 1.7857 - val_loss: 1.7148 - val_mae: 1.7148\n",
      "Epoch 18/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7667 - mae: 1.7667 - val_loss: 1.7052 - val_mae: 1.7052\n",
      "Epoch 19/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7621 - mae: 1.7621 - val_loss: 1.7090 - val_mae: 1.7090\n",
      "Epoch 20/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.7510 - mae: 1.7510 - val_loss: 1.6977 - val_mae: 1.6977\n",
      "Epoch 21/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7507 - mae: 1.7507 - val_loss: 1.6959 - val_mae: 1.6959\n",
      "Epoch 22/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7496 - mae: 1.7496 - val_loss: 1.6883 - val_mae: 1.6883\n",
      "Epoch 23/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7385 - mae: 1.7385 - val_loss: 1.6959 - val_mae: 1.6959\n",
      "Epoch 24/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7675 - mae: 1.7675 - val_loss: 1.6784 - val_mae: 1.6784\n",
      "Epoch 25/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7234 - mae: 1.7234 - val_loss: 1.6697 - val_mae: 1.6697\n",
      "Epoch 26/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7172 - mae: 1.7172 - val_loss: 1.6570 - val_mae: 1.6570\n",
      "Epoch 27/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7131 - mae: 1.7131 - val_loss: 1.6716 - val_mae: 1.6716\n",
      "Epoch 28/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.7035 - mae: 1.7035 - val_loss: 1.6473 - val_mae: 1.6473\n",
      "Epoch 29/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6855 - mae: 1.6855 - val_loss: 1.6154 - val_mae: 1.6154\n",
      "Epoch 30/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6593 - mae: 1.6593 - val_loss: 1.6123 - val_mae: 1.6123\n",
      "Epoch 31/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6512 - mae: 1.6512 - val_loss: 1.5974 - val_mae: 1.5974\n",
      "Epoch 32/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6396 - mae: 1.6396 - val_loss: 1.5831 - val_mae: 1.5831\n",
      "Epoch 33/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6119 - mae: 1.6119 - val_loss: 1.5358 - val_mae: 1.5358\n",
      "Epoch 34/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5846 - mae: 1.5846 - val_loss: 1.5712 - val_mae: 1.5712\n",
      "Epoch 35/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5969 - mae: 1.5969 - val_loss: 1.5962 - val_mae: 1.5962\n",
      "Epoch 36/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6125 - mae: 1.6125 - val_loss: 1.5622 - val_mae: 1.5622\n",
      "Epoch 37/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.6150 - mae: 1.6150 - val_loss: 1.5275 - val_mae: 1.5275\n",
      "Epoch 38/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5848 - mae: 1.5848 - val_loss: 1.5089 - val_mae: 1.5089\n",
      "Epoch 39/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5521 - mae: 1.5521 - val_loss: 1.5061 - val_mae: 1.5061\n",
      "Epoch 40/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5574 - mae: 1.5574 - val_loss: 1.5687 - val_mae: 1.5687\n",
      "Epoch 41/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6211 - mae: 1.6211 - val_loss: 1.5958 - val_mae: 1.5958\n",
      "Epoch 42/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6058 - mae: 1.6058 - val_loss: 1.5153 - val_mae: 1.5153\n",
      "Epoch 43/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5533 - mae: 1.5533 - val_loss: 1.5000 - val_mae: 1.5000\n",
      "Epoch 44/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5574 - mae: 1.5574 - val_loss: 1.6407 - val_mae: 1.6407\n",
      "Epoch 45/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5591 - mae: 1.5591 - val_loss: 1.4964 - val_mae: 1.4964\n",
      "Epoch 46/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5352 - mae: 1.5352 - val_loss: 1.5028 - val_mae: 1.5028\n",
      "Epoch 47/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5351 - mae: 1.5351 - val_loss: 1.5061 - val_mae: 1.5061\n",
      "Epoch 48/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5447 - mae: 1.5447 - val_loss: 1.4959 - val_mae: 1.4959\n",
      "Epoch 49/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5598 - mae: 1.5598 - val_loss: 1.6030 - val_mae: 1.6030\n",
      "Epoch 50/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.6083 - mae: 1.6083 - val_loss: 1.4877 - val_mae: 1.4877\n",
      "Epoch 51/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5592 - mae: 1.5592 - val_loss: 1.4871 - val_mae: 1.4871\n",
      "Epoch 52/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5346 - mae: 1.5346 - val_loss: 1.4877 - val_mae: 1.4877\n",
      "Epoch 53/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5120 - mae: 1.5120 - val_loss: 1.4771 - val_mae: 1.4771\n",
      "Epoch 54/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5163 - mae: 1.5163 - val_loss: 1.4865 - val_mae: 1.4865\n",
      "Epoch 55/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5237 - mae: 1.5237 - val_loss: 1.4720 - val_mae: 1.4720\n",
      "Epoch 56/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5361 - mae: 1.5361 - val_loss: 1.5324 - val_mae: 1.5324\n",
      "Epoch 57/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5148 - mae: 1.5148 - val_loss: 1.4818 - val_mae: 1.4818\n",
      "Epoch 58/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5036 - mae: 1.5036 - val_loss: 1.5393 - val_mae: 1.5393\n",
      "Epoch 59/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5353 - mae: 1.5353 - val_loss: 1.5407 - val_mae: 1.5407\n",
      "Epoch 60/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.5359 - mae: 1.5359 - val_loss: 1.4862 - val_mae: 1.4862\n",
      "Epoch 61/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5289 - mae: 1.5289 - val_loss: 1.4645 - val_mae: 1.4645\n",
      "Epoch 62/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4927 - mae: 1.4927 - val_loss: 1.4581 - val_mae: 1.4581\n",
      "Epoch 63/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4894 - mae: 1.4894 - val_loss: 1.5510 - val_mae: 1.5510\n",
      "Epoch 64/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5098 - mae: 1.5098 - val_loss: 1.4892 - val_mae: 1.4892\n",
      "Epoch 65/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5008 - mae: 1.5008 - val_loss: 1.4758 - val_mae: 1.4758\n",
      "Epoch 66/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5140 - mae: 1.5140 - val_loss: 1.4830 - val_mae: 1.4830\n",
      "Epoch 67/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5033 - mae: 1.5033 - val_loss: 1.4999 - val_mae: 1.4999\n",
      "Epoch 68/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5196 - mae: 1.5196 - val_loss: 1.4456 - val_mae: 1.4456\n",
      "Epoch 69/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4746 - mae: 1.4746 - val_loss: 1.4880 - val_mae: 1.4880\n",
      "Epoch 70/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4991 - mae: 1.4991 - val_loss: 1.5535 - val_mae: 1.5535\n",
      "Epoch 71/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4853 - mae: 1.4853 - val_loss: 1.5261 - val_mae: 1.5261\n",
      "Epoch 72/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4782 - mae: 1.4782 - val_loss: 1.4456 - val_mae: 1.4456\n",
      "Epoch 73/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4898 - mae: 1.4898 - val_loss: 1.4842 - val_mae: 1.4842\n",
      "Epoch 74/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4632 - mae: 1.4632 - val_loss: 1.4681 - val_mae: 1.4681\n",
      "Epoch 75/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4661 - mae: 1.4661 - val_loss: 1.4543 - val_mae: 1.4543\n",
      "Epoch 76/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5121 - mae: 1.5121 - val_loss: 1.5298 - val_mae: 1.5298\n",
      "Epoch 77/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.5248 - mae: 1.5248 - val_loss: 1.4457 - val_mae: 1.4457\n",
      "Epoch 78/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4477 - mae: 1.4477 - val_loss: 1.4529 - val_mae: 1.4529\n",
      "Epoch 79/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4570 - mae: 1.4570 - val_loss: 1.4808 - val_mae: 1.4808\n",
      "Epoch 80/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4545 - mae: 1.4545 - val_loss: 1.4522 - val_mae: 1.4522\n",
      "Epoch 81/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4463 - mae: 1.4463 - val_loss: 1.4352 - val_mae: 1.4352\n",
      "Epoch 82/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4534 - mae: 1.4534 - val_loss: 1.4368 - val_mae: 1.4368\n",
      "Epoch 83/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4524 - mae: 1.4524 - val_loss: 1.4718 - val_mae: 1.4718\n",
      "Epoch 84/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4491 - mae: 1.4491 - val_loss: 1.4350 - val_mae: 1.4350\n",
      "Epoch 85/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4414 - mae: 1.4414 - val_loss: 1.4557 - val_mae: 1.4557\n",
      "Epoch 86/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4456 - mae: 1.4456 - val_loss: 1.4451 - val_mae: 1.4451\n",
      "Epoch 87/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4821 - mae: 1.4821 - val_loss: 1.4967 - val_mae: 1.4967\n",
      "Epoch 88/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4795 - mae: 1.4795 - val_loss: 1.4628 - val_mae: 1.4628\n",
      "Epoch 89/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4593 - mae: 1.4593 - val_loss: 1.4441 - val_mae: 1.4441\n",
      "Epoch 90/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4338 - mae: 1.4338 - val_loss: 1.4997 - val_mae: 1.4997\n",
      "Epoch 91/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4723 - mae: 1.4723 - val_loss: 1.4458 - val_mae: 1.4458\n",
      "Epoch 92/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4454 - mae: 1.4454 - val_loss: 1.4320 - val_mae: 1.4320\n",
      "Epoch 93/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4318 - mae: 1.4318 - val_loss: 1.4573 - val_mae: 1.4573\n",
      "Epoch 94/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4498 - mae: 1.4498 - val_loss: 1.4374 - val_mae: 1.4374\n",
      "Epoch 95/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4423 - mae: 1.4423 - val_loss: 1.4469 - val_mae: 1.4469\n",
      "Epoch 96/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4615 - mae: 1.4615 - val_loss: 1.4313 - val_mae: 1.4313\n",
      "Epoch 97/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4372 - mae: 1.4372 - val_loss: 1.4725 - val_mae: 1.4725\n",
      "Epoch 98/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4558 - mae: 1.4558 - val_loss: 1.4990 - val_mae: 1.4990\n",
      "Epoch 99/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4374 - mae: 1.4374 - val_loss: 1.4493 - val_mae: 1.4493\n",
      "Epoch 100/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4677 - mae: 1.4677 - val_loss: 1.4689 - val_mae: 1.4689\n",
      "Epoch 101/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4571 - mae: 1.4571 - val_loss: 1.4846 - val_mae: 1.4846\n",
      "Epoch 102/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4598 - mae: 1.4598 - val_loss: 1.4526 - val_mae: 1.4526\n",
      "Epoch 103/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4567 - mae: 1.4567 - val_loss: 1.4309 - val_mae: 1.4309\n",
      "Epoch 104/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4378 - mae: 1.4378 - val_loss: 1.4502 - val_mae: 1.4502\n",
      "Epoch 105/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4487 - mae: 1.4487 - val_loss: 1.4230 - val_mae: 1.4230\n",
      "Epoch 106/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4381 - mae: 1.4381 - val_loss: 1.4531 - val_mae: 1.4531\n",
      "Epoch 107/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4677 - mae: 1.4677 - val_loss: 1.4388 - val_mae: 1.4388\n",
      "Epoch 108/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4278 - mae: 1.4278 - val_loss: 1.4438 - val_mae: 1.4438\n",
      "Epoch 109/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4323 - mae: 1.4323 - val_loss: 1.4433 - val_mae: 1.4433\n",
      "Epoch 110/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4377 - mae: 1.4377 - val_loss: 1.4424 - val_mae: 1.4424\n",
      "Epoch 111/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4539 - mae: 1.4539 - val_loss: 1.4747 - val_mae: 1.4747\n",
      "Epoch 112/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4320 - mae: 1.4320 - val_loss: 1.4380 - val_mae: 1.4380\n",
      "Epoch 113/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4339 - mae: 1.4339 - val_loss: 1.4522 - val_mae: 1.4522\n",
      "Epoch 114/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4494 - mae: 1.4494 - val_loss: 1.4590 - val_mae: 1.4590\n",
      "Epoch 115/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4426 - mae: 1.4426 - val_loss: 1.4573 - val_mae: 1.4573\n",
      "Epoch 116/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4376 - mae: 1.4376 - val_loss: 1.4488 - val_mae: 1.4488\n",
      "Epoch 117/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4185 - mae: 1.4185 - val_loss: 1.5389 - val_mae: 1.5389\n",
      "Epoch 118/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4434 - mae: 1.4434 - val_loss: 1.4455 - val_mae: 1.4455\n",
      "Epoch 119/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4369 - mae: 1.4369 - val_loss: 1.4452 - val_mae: 1.4452\n",
      "Epoch 120/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4436 - mae: 1.4436 - val_loss: 1.4280 - val_mae: 1.4280\n",
      "Epoch 121/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4327 - mae: 1.4327 - val_loss: 1.4909 - val_mae: 1.4909\n",
      "Epoch 122/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4304 - mae: 1.4304 - val_loss: 1.4408 - val_mae: 1.4408\n",
      "Epoch 123/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4129 - mae: 1.4129 - val_loss: 1.4394 - val_mae: 1.4394\n",
      "Epoch 124/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4256 - mae: 1.4256 - val_loss: 1.4955 - val_mae: 1.4955\n",
      "Epoch 125/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4461 - mae: 1.4461 - val_loss: 1.4322 - val_mae: 1.4322\n",
      "Epoch 126/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4068 - mae: 1.4068 - val_loss: 1.4333 - val_mae: 1.4333\n",
      "Epoch 127/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3996 - mae: 1.3996 - val_loss: 1.4296 - val_mae: 1.4296\n",
      "Epoch 128/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4007 - mae: 1.4007 - val_loss: 1.4306 - val_mae: 1.4306\n",
      "Epoch 129/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4010 - mae: 1.4010 - val_loss: 1.4273 - val_mae: 1.4273\n",
      "Epoch 130/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4016 - mae: 1.4016 - val_loss: 1.4280 - val_mae: 1.4280\n",
      "Epoch 131/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3989 - mae: 1.3989 - val_loss: 1.4287 - val_mae: 1.4287\n",
      "Epoch 132/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4013 - mae: 1.4013 - val_loss: 1.4263 - val_mae: 1.4263\n",
      "Epoch 133/1000\n",
      "22/22 [==============================] - ETA: 0s - loss: 1.4683 - mae: 1.468 - 0s 1ms/step - loss: 1.4100 - mae: 1.4100 - val_loss: 1.4341 - val_mae: 1.4341\n",
      "Epoch 134/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4027 - mae: 1.4027 - val_loss: 1.4328 - val_mae: 1.4328\n",
      "Epoch 135/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3997 - mae: 1.3997 - val_loss: 1.4312 - val_mae: 1.4312\n",
      "Epoch 136/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4034 - mae: 1.4034 - val_loss: 1.4279 - val_mae: 1.4279\n",
      "Epoch 137/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4035 - mae: 1.4035 - val_loss: 1.4314 - val_mae: 1.4314\n",
      "Epoch 138/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4082 - mae: 1.4082 - val_loss: 1.4375 - val_mae: 1.4375\n",
      "Epoch 139/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4018 - mae: 1.4018 - val_loss: 1.4282 - val_mae: 1.4282\n",
      "Epoch 140/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4007 - mae: 1.4007 - val_loss: 1.4309 - val_mae: 1.4309\n",
      "Epoch 141/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4006 - mae: 1.4006 - val_loss: 1.4261 - val_mae: 1.4261\n",
      "Epoch 142/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4022 - mae: 1.4022 - val_loss: 1.4218 - val_mae: 1.4218\n",
      "Epoch 143/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4044 - mae: 1.4044 - val_loss: 1.4345 - val_mae: 1.4345\n",
      "Epoch 144/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4023 - mae: 1.4023 - val_loss: 1.4313 - val_mae: 1.4313\n",
      "Epoch 145/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4000 - mae: 1.4000 - val_loss: 1.4228 - val_mae: 1.4228\n",
      "Epoch 146/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4089 - mae: 1.4089 - val_loss: 1.4249 - val_mae: 1.4249\n",
      "Epoch 147/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4004 - mae: 1.4004 - val_loss: 1.4294 - val_mae: 1.4294\n",
      "Epoch 148/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4041 - mae: 1.4041 - val_loss: 1.4282 - val_mae: 1.4282\n",
      "Epoch 149/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4010 - mae: 1.4010 - val_loss: 1.4281 - val_mae: 1.4281\n",
      "Epoch 150/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3992 - mae: 1.3992 - val_loss: 1.4293 - val_mae: 1.4293\n",
      "Epoch 151/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3984 - mae: 1.3984 - val_loss: 1.4269 - val_mae: 1.4269\n",
      "Epoch 152/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4003 - mae: 1.4003 - val_loss: 1.4306 - val_mae: 1.4306\n",
      "Epoch 153/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3955 - mae: 1.3955 - val_loss: 1.4234 - val_mae: 1.4234\n",
      "Epoch 154/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3984 - mae: 1.3984 - val_loss: 1.4232 - val_mae: 1.4232\n",
      "Epoch 155/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3969 - mae: 1.3969 - val_loss: 1.4340 - val_mae: 1.4340\n",
      "Epoch 156/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3961 - mae: 1.3961 - val_loss: 1.4251 - val_mae: 1.4251\n",
      "Epoch 157/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4038 - mae: 1.4038 - val_loss: 1.4375 - val_mae: 1.4375\n",
      "Epoch 158/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3935 - mae: 1.3935 - val_loss: 1.4239 - val_mae: 1.4239\n",
      "Epoch 159/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.4067 - mae: 1.4067 - val_loss: 1.4280 - val_mae: 1.4280\n",
      "Epoch 160/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.4000 - mae: 1.4000 - val_loss: 1.4327 - val_mae: 1.4327\n",
      "Epoch 161/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3984 - mae: 1.3984 - val_loss: 1.4224 - val_mae: 1.4224\n",
      "Epoch 162/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3982 - mae: 1.3982 - val_loss: 1.4230 - val_mae: 1.4230\n",
      "Epoch 163/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3932 - mae: 1.3932 - val_loss: 1.4234 - val_mae: 1.4234\n",
      "Epoch 164/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 1.4265 - val_mae: 1.4265\n",
      "Epoch 165/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3933 - mae: 1.3933 - val_loss: 1.4262 - val_mae: 1.4262\n",
      "Epoch 166/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3944 - mae: 1.3944 - val_loss: 1.4253 - val_mae: 1.4253\n",
      "Epoch 167/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3926 - mae: 1.3926 - val_loss: 1.4258 - val_mae: 1.4258\n",
      "Epoch 168/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3928 - mae: 1.3928 - val_loss: 1.4253 - val_mae: 1.4253\n",
      "Epoch 169/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3939 - mae: 1.3939 - val_loss: 1.4259 - val_mae: 1.4259\n",
      "Epoch 170/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3938 - mae: 1.3938 - val_loss: 1.4226 - val_mae: 1.4226\n",
      "Epoch 171/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3933 - mae: 1.3933 - val_loss: 1.4266 - val_mae: 1.4266\n",
      "Epoch 172/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3926 - mae: 1.3926 - val_loss: 1.4248 - val_mae: 1.4248\n",
      "Epoch 173/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3929 - mae: 1.3929 - val_loss: 1.4251 - val_mae: 1.4251\n",
      "Epoch 174/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3926 - mae: 1.3926 - val_loss: 1.4242 - val_mae: 1.4242\n",
      "Epoch 175/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3933 - mae: 1.3933 - val_loss: 1.4242 - val_mae: 1.4242\n",
      "Epoch 176/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3928 - mae: 1.3928 - val_loss: 1.4254 - val_mae: 1.4254\n",
      "Epoch 177/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3928 - mae: 1.3928 - val_loss: 1.4248 - val_mae: 1.4248\n",
      "Epoch 178/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3935 - mae: 1.3935 - val_loss: 1.4241 - val_mae: 1.4241\n",
      "Epoch 179/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 1.4249 - val_mae: 1.4249\n",
      "Epoch 180/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 1.4253 - val_mae: 1.4253\n",
      "Epoch 181/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3937 - mae: 1.3937 - val_loss: 1.4222 - val_mae: 1.4222\n",
      "Epoch 182/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3924 - mae: 1.3924 - val_loss: 1.4248 - val_mae: 1.4248\n",
      "Epoch 183/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3925 - mae: 1.3925 - val_loss: 1.4258 - val_mae: 1.4258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3922 - mae: 1.3922 - val_loss: 1.4257 - val_mae: 1.4257\n",
      "Epoch 185/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3923 - mae: 1.3923 - val_loss: 1.4247 - val_mae: 1.4247\n",
      "Epoch 186/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3921 - mae: 1.3921 - val_loss: 1.4255 - val_mae: 1.4255\n",
      "Epoch 187/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3917 - mae: 1.3917 - val_loss: 1.4251 - val_mae: 1.4251\n",
      "Epoch 188/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3920 - mae: 1.3920 - val_loss: 1.4245 - val_mae: 1.4245\n",
      "Epoch 189/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3918 - mae: 1.3918 - val_loss: 1.4250 - val_mae: 1.4250\n",
      "Epoch 190/1000\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 1.3921 - mae: 1.3921 - val_loss: 1.4254 - val_mae: 1.4254\n",
      "Epoch 191/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3917 - mae: 1.3917 - val_loss: 1.4250 - val_mae: 1.4250\n",
      "Epoch 192/1000\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3919 - mae: 1.3919 - val_loss: 1.4247 - val_mae: 1.4247\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(X_train, Y_train, validation_split=0.3, epochs=1000, batch_size=32, verbose=1, callbacks=[early_stopping_callback, checkpointer, rlrp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1f444f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제나이: 4.000, 예상나이: 4.176\n",
      "실제나이: 11.000, 예상나이: 9.096\n",
      "실제나이: 8.000, 예상나이: 11.896\n",
      "실제나이: 10.000, 예상나이: 8.639\n",
      "실제나이: 5.000, 예상나이: 5.235\n",
      "실제나이: 11.000, 예상나이: 11.731\n",
      "실제나이: 10.000, 예상나이: 9.720\n",
      "실제나이: 8.000, 예상나이: 8.011\n",
      "실제나이: 7.000, 예상나이: 7.571\n",
      "실제나이: 11.000, 예상나이: 9.410\n",
      "실제나이: 12.000, 예상나이: 9.643\n",
      "실제나이: 9.000, 예상나이: 9.907\n",
      "실제나이: 5.000, 예상나이: 5.470\n",
      "실제나이: 13.000, 예상나이: 12.212\n",
      "실제나이: 6.000, 예상나이: 7.186\n",
      "실제나이: 6.000, 예상나이: 6.731\n",
      "실제나이: 6.000, 예상나이: 8.083\n",
      "실제나이: 8.000, 예상나이: 8.087\n",
      "실제나이: 8.000, 예상나이: 9.211\n",
      "실제나이: 9.000, 예상나이: 10.414\n"
     ]
    }
   ],
   "source": [
    "# 예측 값과 실제 값의 비교\n",
    "Y_prediction = model.predict(X_test).flatten()\n",
    "# flatten : 데이터 배열이 몇 차원이든 모두 1차원으로 바꿔 읽기 쉽게 해 주는 함수\n",
    "\n",
    "# 10개 실제값과 예측값 비교\n",
    "for i in range(20):\n",
    "    label = Y_test.values[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제나이: {:.3f}, 예상나이: {:.3f}\".format(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd8709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181da17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9af44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
