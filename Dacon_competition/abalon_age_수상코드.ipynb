{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d671c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "데이콘 변수 설명\n",
    "\n",
    "Gender : 전복 성별\n",
    "Lenght : 전복 길이\n",
    "Diameter : 전복 둘레\n",
    "Height : 전복 키 \n",
    "Whole : Weight : 전복 전체 무게\n",
    "Shucked Weight : 껍질을 제외한 무게\n",
    "Viscra Weight : 내장 무게\n",
    "Shell Weight : 껍질 무게\n",
    "Target : 전복 나이\n",
    "-----------------------------------------------------\n",
    "\n",
    "※변수에 대한 다른 생각\n",
    "\n",
    "Lenght / 연속 / mm / 가장 긴 껍질 측정 (장축)\n",
    "Diameter / 연속 / mm / 길이에 수직 (직경 / 단축)\n",
    "Height / 연속 / mm / 높이 , 껍질에 고기 포함\n",
    "Whole Weight : 전복 전체 무게 (whole abalone weight)\n",
    "Shucked Weight : 껍질을 제외한 무게 (weight of meat only)\n",
    "Viscra Weight : 내장 무게 (gut weight, after bleeding)\n",
    "Shell Weight : 껍질 무게 (weight after being dried)\n",
    "------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556d4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c706ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)\n",
    "\n",
    "train_X = train.drop('Target', axis=1)\n",
    "train_y = train.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf096a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.get_dummies(data = train_X, columns = ['Gender'], prefix = 'Gender')\n",
    "test = pd.get_dummies(data = test, columns = ['Gender'], prefix = 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e411461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Shucked Weight</th>\n",
       "      <th>Viscra Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Gender_F</th>\n",
       "      <th>Gender_I</th>\n",
       "      <th>Gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.605</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.1140</td>\n",
       "      <td>0.3925</td>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.430</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.3780</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.580</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.3165</td>\n",
       "      <td>0.5305</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.535</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.2705</td>\n",
       "      <td>0.5480</td>\n",
       "      <td>0.3265</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.310</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>0.190</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>0.525</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.7745</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>0.445</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.4355</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>0.1095</td>\n",
       "      <td>0.1195</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.8325</td>\n",
       "      <td>0.8300</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.4400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lenght  Diameter  Height  Whole Weight  Shucked Weight  Viscra Weight  \\\n",
       "0      0.605     0.470   0.115        1.1140          0.3925         0.2910   \n",
       "1      0.430     0.315   0.095        0.3780          0.1750         0.0800   \n",
       "2      0.580     0.490   0.195        1.3165          0.5305         0.2540   \n",
       "3      0.535     0.405   0.175        1.2705          0.5480         0.3265   \n",
       "4      0.310     0.235   0.090        0.1270          0.0480         0.0310   \n",
       "...      ...       ...     ...           ...             ...            ...   \n",
       "1248   0.190     0.145   0.040        0.0380          0.0165         0.0065   \n",
       "1249   0.395     0.310   0.085        0.3170          0.1530         0.0505   \n",
       "1250   0.525     0.410   0.115        0.7745          0.4160         0.1630   \n",
       "1251   0.445     0.335   0.110        0.4355          0.2025         0.1095   \n",
       "1252   0.750     0.550   0.195        1.8325          0.8300         0.3660   \n",
       "\n",
       "      Shell Weight  Gender_F  Gender_I  Gender_M  \n",
       "0           0.3100         0         0         1  \n",
       "1           0.1045         0         1         0  \n",
       "2           0.4100         0         1         0  \n",
       "3           0.3370         0         0         1  \n",
       "4           0.0400         0         1         0  \n",
       "...            ...       ...       ...       ...  \n",
       "1248        0.0150         0         1         0  \n",
       "1249        0.0935         0         1         0  \n",
       "1250        0.1800         1         0         0  \n",
       "1251        0.1195         1         0         0  \n",
       "1252        0.4400         1         0         0  \n",
       "\n",
       "[1253 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72b80bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전복 전체 무게 중 불필요한 물질들의 값 ( 이물질 / 핏물 등)\n",
    "foreign_body = train_X['Whole Weight'] - (train_X['Shucked Weight'] + train_X['Viscra Weight'] + train_X['Shell Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ebaae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.1205\n",
       "1       0.0185\n",
       "2       0.1220\n",
       "3       0.0590\n",
       "4       0.0080\n",
       "         ...  \n",
       "1248    0.0000\n",
       "1249    0.0200\n",
       "1250    0.0155\n",
       "1251    0.0040\n",
       "1252    0.1965\n",
       "Length: 1253, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1313c952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06749999999999998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_body[47] # 음수??"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7eceecf",
   "metadata": {},
   "source": [
    "전체 무게가 살 + 내장 + 껍질보다 적게 나가는 경우???   \n",
    "    \n",
    "전체 무게 - (살 + 내장 + 껍질)  = 핏물 + 껍데기 물 이물질 \n",
    "\n",
    "전체 무게 - (살 + 내장 + 껍질) > 0 => 핏물 껍데기 물이 있다\n",
    "\n",
    "전체 무게 - (살 + 내장 + 껍질) < 0 => 핏물 껍데기 물이 없다??? => 말이 안돼 => 0으로 처리...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9747231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전복 전체 무게 대비 불필요한 물질들의 값 ( 이물질 / 핏물 등)\n",
    "foreign_body = train_X['Whole Weight'] - (train_X['Shucked Weight'] + train_X['Viscra Weight'] + train_X['Shell Weight'])\n",
    "train_X['foreign body'] = foreign_body\n",
    "train_X.loc[train_X['foreign body'] < 0 , 'foreign body'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3ff103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋에도 적용\n",
    "foreign_body = test['Whole Weight'] - (test['Shucked Weight'] + test['Viscra Weight'] + test['Shell Weight'])\n",
    "test['foreign body'] = foreign_body\n",
    "test.loc[test['foreign body'] < 0 , 'foreign body'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d3b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "089c91b5",
   "metadata": {},
   "source": [
    "Diameter : 전복의 둘레\n",
    " => 둘레라는 것이 원의 둘레(2πr)의 둘레가 맞는가? => 그럼 Lenght보다 작은게 말이 안돼 => Diameter : 전복의 직경?\n",
    " ==> 직경이라면 Lenght는 장축의 지름 / Diameter는 단축의 지름\n",
    " ===> 타원(전복 껍데기)의 둘레 및 넓이, 부피까지 구할 수 있지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef574a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 껍질의 넓이 ( a * b * π)\n",
    "area = 0.5 * train_X['Lenght'] * 0.5 * train_X['Diameter'] * np.pi\n",
    "train_X['Area'] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b34ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 껍질의 넓이 \n",
    "area = 0.5 * test['Lenght'] * 0.5 * test['Diameter'] * np.pi\n",
    "test['Area'] = area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ceca508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 껍질의 둘레 (근사) ( 2π*(0.5 * √(a^2 + b^2)))\n",
    "perimeter = np.pi * np.sqrt(0.5 * ((train_X['Lenght'] ** 2) + (train_X['Diameter'] ** 2)))\n",
    "train_X['Perimeter'] = perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e3bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 껍질의 둘레\n",
    "perimeter = np.pi * np.sqrt(0.5 * ((test['Lenght'] ** 2) + (test['Diameter'] ** 2)))\n",
    "test['Perimeter'] = perimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308dc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7719a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22b73e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "28/28 [==============================] - 1s 9ms/step - loss: 6.0431 - mae: 6.0431 - val_loss: 2.0536 - val_mae: 2.0536 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 2.1157 - mae: 2.1157 - val_loss: 1.9256 - val_mae: 1.9256 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 2.0257 - mae: 2.0257 - val_loss: 1.9054 - val_mae: 1.9054 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.9392 - mae: 1.9392 - val_loss: 1.9032 - val_mae: 1.9032 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.8962 - mae: 1.8962 - val_loss: 1.8910 - val_mae: 1.8910 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.8416 - mae: 1.8416 - val_loss: 1.9017 - val_mae: 1.9017 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.8365 - mae: 1.8365 - val_loss: 1.9299 - val_mae: 1.9299 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.8610 - mae: 1.8610 - val_loss: 1.8876 - val_mae: 1.8876 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.8617 - mae: 1.8617 - val_loss: 1.8815 - val_mae: 1.8815 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.8257 - mae: 1.8257 - val_loss: 1.8946 - val_mae: 1.8946 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7786 - mae: 1.7786 - val_loss: 1.8676 - val_mae: 1.8676 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.7928 - mae: 1.7928 - val_loss: 1.8656 - val_mae: 1.8656 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7782 - mae: 1.7782 - val_loss: 1.9109 - val_mae: 1.9109 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.7948 - mae: 1.7948 - val_loss: 1.8559 - val_mae: 1.8559 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7611 - mae: 1.7611 - val_loss: 1.8575 - val_mae: 1.8575 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.7654 - mae: 1.7654 - val_loss: 1.8483 - val_mae: 1.8483 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.7669 - mae: 1.7669 - val_loss: 1.8356 - val_mae: 1.8356 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.7236 - mae: 1.7236 - val_loss: 1.8305 - val_mae: 1.8305 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.7438 - mae: 1.7438 - val_loss: 1.9207 - val_mae: 1.9207 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6944 - mae: 1.6944 - val_loss: 1.8069 - val_mae: 1.8069 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6812 - mae: 1.6812 - val_loss: 1.8265 - val_mae: 1.8265 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6562 - mae: 1.6562 - val_loss: 1.8054 - val_mae: 1.8054 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6796 - mae: 1.6796 - val_loss: 1.7884 - val_mae: 1.7884 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6724 - mae: 1.6724 - val_loss: 1.8143 - val_mae: 1.8143 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6389 - mae: 1.6389 - val_loss: 1.7594 - val_mae: 1.7594 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6629 - mae: 1.6629 - val_loss: 1.7511 - val_mae: 1.7511 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6364 - mae: 1.6364 - val_loss: 1.7499 - val_mae: 1.7499 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6140 - mae: 1.6140 - val_loss: 1.7373 - val_mae: 1.7373 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6362 - mae: 1.6362 - val_loss: 1.7282 - val_mae: 1.7282 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6021 - mae: 1.6021 - val_loss: 1.7352 - val_mae: 1.7352 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5909 - mae: 1.5909 - val_loss: 1.7105 - val_mae: 1.7105 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6084 - mae: 1.6084 - val_loss: 1.7137 - val_mae: 1.7137 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5849 - mae: 1.5849 - val_loss: 1.7231 - val_mae: 1.7231 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.5826 - mae: 1.5826 - val_loss: 1.6981 - val_mae: 1.6981 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5884 - mae: 1.5884 - val_loss: 1.9201 - val_mae: 1.9201 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.6050 - mae: 1.6050 - val_loss: 1.7094 - val_mae: 1.7094 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5529 - mae: 1.5529 - val_loss: 1.7035 - val_mae: 1.7035 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5709 - mae: 1.5709 - val_loss: 1.7340 - val_mae: 1.7340 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.5688 - mae: 1.5688 - val_loss: 1.6798 - val_mae: 1.6798 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5661 - mae: 1.5661 - val_loss: 1.7872 - val_mae: 1.7872 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5560 - mae: 1.5560 - val_loss: 1.7187 - val_mae: 1.7187 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5718 - mae: 1.5718 - val_loss: 1.6864 - val_mae: 1.6864 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5654 - mae: 1.5654 - val_loss: 1.6861 - val_mae: 1.6861 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5909 - mae: 1.5909 - val_loss: 1.6852 - val_mae: 1.6852 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5569 - mae: 1.5569 - val_loss: 1.7710 - val_mae: 1.7710 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5596 - mae: 1.5596 - val_loss: 1.6992 - val_mae: 1.6992 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5154 - mae: 1.5154 - val_loss: 1.7421 - val_mae: 1.7421 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5333 - mae: 1.5333 - val_loss: 1.7853 - val_mae: 1.7853 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5193 - mae: 1.5193 - val_loss: 1.6842 - val_mae: 1.6842 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5405 - mae: 1.5405 - val_loss: 1.6947 - val_mae: 1.6947 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5326 - mae: 1.5326 - val_loss: 1.6877 - val_mae: 1.6877 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5347 - mae: 1.5347 - val_loss: 1.7110 - val_mae: 1.7110 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.5084 - mae: 1.5084 - val_loss: 1.6952 - val_mae: 1.6952 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5200 - mae: 1.5200 - val_loss: 1.7564 - val_mae: 1.7564 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5503 - mae: 1.5503 - val_loss: 1.7179 - val_mae: 1.7179 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5320 - mae: 1.5320 - val_loss: 1.6983 - val_mae: 1.6983 - lr: 0.0010\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5081 - mae: 1.5081 - val_loss: 1.6918 - val_mae: 1.6918 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5271 - mae: 1.5271 - val_loss: 1.7083 - val_mae: 1.7083 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5325 - mae: 1.5325 - val_loss: 1.6624 - val_mae: 1.6624 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5325 - mae: 1.5325 - val_loss: 1.6629 - val_mae: 1.6629 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5103 - mae: 1.5103 - val_loss: 1.7916 - val_mae: 1.7916 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5118 - mae: 1.5118 - val_loss: 1.7131 - val_mae: 1.7131 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5134 - mae: 1.5134 - val_loss: 1.7817 - val_mae: 1.7817 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5167 - mae: 1.5167 - val_loss: 1.6656 - val_mae: 1.6656 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5139 - mae: 1.5139 - val_loss: 1.6959 - val_mae: 1.6959 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5089 - mae: 1.5089 - val_loss: 1.6627 - val_mae: 1.6627 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5230 - mae: 1.5230 - val_loss: 1.6688 - val_mae: 1.6688 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5345 - mae: 1.5345 - val_loss: 1.6764 - val_mae: 1.6764 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5219 - mae: 1.5219 - val_loss: 1.6859 - val_mae: 1.6859 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5030 - mae: 1.5030 - val_loss: 1.7668 - val_mae: 1.7668 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5075 - mae: 1.5075 - val_loss: 1.7117 - val_mae: 1.7117 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5042 - mae: 1.5042 - val_loss: 1.7223 - val_mae: 1.7223 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4806 - mae: 1.4806 - val_loss: 1.6671 - val_mae: 1.6671 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5012 - mae: 1.5012 - val_loss: 1.6849 - val_mae: 1.6849 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4861 - mae: 1.4861 - val_loss: 1.6523 - val_mae: 1.6523 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5063 - mae: 1.5063 - val_loss: 1.6858 - val_mae: 1.6858 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5252 - mae: 1.5252 - val_loss: 1.6882 - val_mae: 1.6882 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4938 - mae: 1.4938 - val_loss: 1.6524 - val_mae: 1.6524 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4981 - mae: 1.4981 - val_loss: 1.6580 - val_mae: 1.6580 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.4927 - mae: 1.4927 - val_loss: 1.6435 - val_mae: 1.6435 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4926 - mae: 1.4926 - val_loss: 1.6814 - val_mae: 1.6814 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5060 - mae: 1.5060 - val_loss: 1.7160 - val_mae: 1.7160 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4809 - mae: 1.4809 - val_loss: 1.6510 - val_mae: 1.6510 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4748 - mae: 1.4748 - val_loss: 1.7091 - val_mae: 1.7091 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5131 - mae: 1.5131 - val_loss: 1.6658 - val_mae: 1.6658 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4677 - mae: 1.4677 - val_loss: 1.6665 - val_mae: 1.6665 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4842 - mae: 1.4842 - val_loss: 1.7052 - val_mae: 1.7052 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4889 - mae: 1.4889 - val_loss: 1.6552 - val_mae: 1.6552 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5016 - mae: 1.5016 - val_loss: 1.7521 - val_mae: 1.7521 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5027 - mae: 1.5027 - val_loss: 1.6812 - val_mae: 1.6812 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4833 - mae: 1.4833 - val_loss: 1.6544 - val_mae: 1.6544 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5075 - mae: 1.5075 - val_loss: 1.6540 - val_mae: 1.6540 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4850 - mae: 1.4850 - val_loss: 1.6445 - val_mae: 1.6445 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4776 - mae: 1.4776 - val_loss: 1.6708 - val_mae: 1.6708 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4719 - mae: 1.4719 - val_loss: 1.7687 - val_mae: 1.7687 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.5018 - mae: 1.5018 - val_loss: 1.6333 - val_mae: 1.6333 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.4803 - mae: 1.4803 - val_loss: 1.6288 - val_mae: 1.6288 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4927 - mae: 1.4927 - val_loss: 1.6610 - val_mae: 1.6610 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4807 - mae: 1.4807 - val_loss: 1.6552 - val_mae: 1.6552 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4591 - mae: 1.4591 - val_loss: 1.7728 - val_mae: 1.7728 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.4839 - mae: 1.4839 - val_loss: 1.6222 - val_mae: 1.6222 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4648 - mae: 1.4648 - val_loss: 1.6524 - val_mae: 1.6524 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4812 - mae: 1.4812 - val_loss: 1.6311 - val_mae: 1.6311 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4678 - mae: 1.4678 - val_loss: 1.6228 - val_mae: 1.6228 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4769 - mae: 1.4769 - val_loss: 1.6270 - val_mae: 1.6270 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4818 - mae: 1.4818 - val_loss: 1.6214 - val_mae: 1.6214 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4825 - mae: 1.4825 - val_loss: 1.6365 - val_mae: 1.6365 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4606 - mae: 1.4606 - val_loss: 1.6644 - val_mae: 1.6644 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4723 - mae: 1.4723 - val_loss: 1.6233 - val_mae: 1.6233 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4752 - mae: 1.4752 - val_loss: 1.6883 - val_mae: 1.6883 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4701 - mae: 1.4701 - val_loss: 1.6272 - val_mae: 1.6272 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4836 - mae: 1.4836 - val_loss: 1.6456 - val_mae: 1.6456 - lr: 0.0010\n",
      "Epoch 113/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4806 - mae: 1.4806 - val_loss: 1.6248 - val_mae: 1.6248 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4668 - mae: 1.4668 - val_loss: 1.6238 - val_mae: 1.6238 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4848 - mae: 1.4848 - val_loss: 1.6199 - val_mae: 1.6199 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4656 - mae: 1.4656 - val_loss: 1.6327 - val_mae: 1.6327 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4725 - mae: 1.4725 - val_loss: 1.6238 - val_mae: 1.6238 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.4691 - mae: 1.4691 - val_loss: 1.6125 - val_mae: 1.6125 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4715 - mae: 1.4715 - val_loss: 1.6650 - val_mae: 1.6650 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4944 - mae: 1.4944 - val_loss: 1.6278 - val_mae: 1.6278 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1.4618 - mae: 1.4618 - val_loss: 1.6038 - val_mae: 1.6038 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4850 - mae: 1.4850 - val_loss: 1.6335 - val_mae: 1.6335 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4551 - mae: 1.4551 - val_loss: 1.6735 - val_mae: 1.6735 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4754 - mae: 1.4754 - val_loss: 1.6191 - val_mae: 1.6191 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4802 - mae: 1.4802 - val_loss: 1.6275 - val_mae: 1.6275 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4432 - mae: 1.4432 - val_loss: 1.6100 - val_mae: 1.6100 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4439 - mae: 1.4439 - val_loss: 1.6241 - val_mae: 1.6241 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4611 - mae: 1.4611 - val_loss: 1.6092 - val_mae: 1.6092 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4561 - mae: 1.4561 - val_loss: 1.6127 - val_mae: 1.6127 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4513 - mae: 1.4513 - val_loss: 1.6052 - val_mae: 1.6052 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4608 - mae: 1.4608 - val_loss: 1.6090 - val_mae: 1.6090 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4863 - mae: 1.4863 - val_loss: 1.6164 - val_mae: 1.6164 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4540 - mae: 1.4540 - val_loss: 1.6597 - val_mae: 1.6597 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4620 - mae: 1.4620 - val_loss: 1.6244 - val_mae: 1.6244 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4425 - mae: 1.4425 - val_loss: 1.7003 - val_mae: 1.7003 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4518 - mae: 1.4518 - val_loss: 1.6424 - val_mae: 1.6424 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4516 - mae: 1.4516 - val_loss: 1.5898 - val_mae: 1.5898 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4456 - mae: 1.4456 - val_loss: 1.6041 - val_mae: 1.6041 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4428 - mae: 1.4428 - val_loss: 1.5967 - val_mae: 1.5967 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4258 - mae: 1.4258 - val_loss: 1.5977 - val_mae: 1.5977 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4527 - mae: 1.4527 - val_loss: 1.6076 - val_mae: 1.6076 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4380 - mae: 1.4380 - val_loss: 1.5788 - val_mae: 1.5788 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4463 - mae: 1.4463 - val_loss: 1.5854 - val_mae: 1.5854 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4507 - mae: 1.4507 - val_loss: 1.5975 - val_mae: 1.5975 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4483 - mae: 1.4483 - val_loss: 1.6089 - val_mae: 1.6089 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4642 - mae: 1.4642 - val_loss: 1.6187 - val_mae: 1.6187 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4553 - mae: 1.4553 - val_loss: 1.6415 - val_mae: 1.6415 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4473 - mae: 1.4473 - val_loss: 1.5772 - val_mae: 1.5772 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4583 - mae: 1.4583 - val_loss: 1.5972 - val_mae: 1.5972 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4592 - mae: 1.4592 - val_loss: 1.5862 - val_mae: 1.5862 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4241 - mae: 1.4241 - val_loss: 1.5782 - val_mae: 1.5782 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4673 - mae: 1.4673 - val_loss: 1.5800 - val_mae: 1.5800 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4218 - mae: 1.4218 - val_loss: 1.6444 - val_mae: 1.6444 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4549 - mae: 1.4549 - val_loss: 1.5927 - val_mae: 1.5927 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4714 - mae: 1.4714 - val_loss: 1.5713 - val_mae: 1.5713 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4293 - mae: 1.4293 - val_loss: 1.5971 - val_mae: 1.5971 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4516 - mae: 1.4516 - val_loss: 1.5842 - val_mae: 1.5842 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4323 - mae: 1.4323 - val_loss: 1.6105 - val_mae: 1.6105 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4545 - mae: 1.4545 - val_loss: 1.6276 - val_mae: 1.6276 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4515 - mae: 1.4515 - val_loss: 1.5811 - val_mae: 1.5811 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4492 - mae: 1.4492 - val_loss: 1.5677 - val_mae: 1.5677 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4444 - mae: 1.4444 - val_loss: 1.6117 - val_mae: 1.6117 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4297 - mae: 1.4297 - val_loss: 1.5745 - val_mae: 1.5745 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4382 - mae: 1.4382 - val_loss: 1.5861 - val_mae: 1.5861 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4240 - mae: 1.4240 - val_loss: 1.5816 - val_mae: 1.5816 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4525 - mae: 1.4525 - val_loss: 1.6606 - val_mae: 1.6606 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4179 - mae: 1.4179 - val_loss: 1.5982 - val_mae: 1.5982 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4016 - mae: 1.4016 - val_loss: 1.5745 - val_mae: 1.5745 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4374 - mae: 1.4374 - val_loss: 1.5602 - val_mae: 1.5602 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4227 - mae: 1.4227 - val_loss: 1.5899 - val_mae: 1.5899 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4294 - mae: 1.4294 - val_loss: 1.5685 - val_mae: 1.5685 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4349 - mae: 1.4349 - val_loss: 1.5730 - val_mae: 1.5730 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4286 - mae: 1.4286 - val_loss: 1.5610 - val_mae: 1.5610 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4523 - mae: 1.4523 - val_loss: 1.5593 - val_mae: 1.5593 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4172 - mae: 1.4172 - val_loss: 1.5650 - val_mae: 1.5650 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4224 - mae: 1.4224 - val_loss: 1.5651 - val_mae: 1.5651 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4271 - mae: 1.4271 - val_loss: 1.5828 - val_mae: 1.5828 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4313 - mae: 1.4313 - val_loss: 1.5447 - val_mae: 1.5447 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4456 - mae: 1.4456 - val_loss: 1.5511 - val_mae: 1.5511 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4275 - mae: 1.4275 - val_loss: 1.5705 - val_mae: 1.5705 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4303 - mae: 1.4303 - val_loss: 1.5740 - val_mae: 1.5740 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4171 - mae: 1.4171 - val_loss: 1.7781 - val_mae: 1.7781 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4542 - mae: 1.4542 - val_loss: 1.5600 - val_mae: 1.5600 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3955 - mae: 1.3955 - val_loss: 1.5587 - val_mae: 1.5587 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4134 - mae: 1.4134 - val_loss: 1.5733 - val_mae: 1.5733 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4150 - mae: 1.4150 - val_loss: 1.5606 - val_mae: 1.5606 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4438 - mae: 1.4438 - val_loss: 1.5512 - val_mae: 1.5512 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4309 - mae: 1.4309 - val_loss: 1.5462 - val_mae: 1.5462 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4329 - mae: 1.4329 - val_loss: 1.5719 - val_mae: 1.5719 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4098 - mae: 1.4098 - val_loss: 1.5946 - val_mae: 1.5946 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4416 - mae: 1.4416 - val_loss: 1.5429 - val_mae: 1.5429 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4214 - mae: 1.4214 - val_loss: 1.5363 - val_mae: 1.5363 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4114 - mae: 1.4114 - val_loss: 1.5476 - val_mae: 1.5476 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4313 - mae: 1.4313 - val_loss: 1.5892 - val_mae: 1.5892 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4246 - mae: 1.4246 - val_loss: 1.5472 - val_mae: 1.5472 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4167 - mae: 1.4167 - val_loss: 1.5922 - val_mae: 1.5922 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4146 - mae: 1.4146 - val_loss: 1.5478 - val_mae: 1.5478 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4068 - mae: 1.4068 - val_loss: 1.5577 - val_mae: 1.5577 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4260 - mae: 1.4260 - val_loss: 1.5707 - val_mae: 1.5707 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4218 - mae: 1.4218 - val_loss: 1.5406 - val_mae: 1.5406 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4104 - mae: 1.4104 - val_loss: 1.5612 - val_mae: 1.5612 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4185 - mae: 1.4185 - val_loss: 1.5663 - val_mae: 1.5663 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4482 - mae: 1.4482 - val_loss: 1.5547 - val_mae: 1.5547 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4074 - mae: 1.4074 - val_loss: 1.5433 - val_mae: 1.5433 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4332 - mae: 1.4332 - val_loss: 1.5525 - val_mae: 1.5525 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4132 - mae: 1.4132 - val_loss: 1.5383 - val_mae: 1.5383 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4171 - mae: 1.4171 - val_loss: 1.5704 - val_mae: 1.5704 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4400 - mae: 1.4400 - val_loss: 1.5799 - val_mae: 1.5799 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4223 - mae: 1.4223 - val_loss: 1.5512 - val_mae: 1.5512 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3952 - mae: 1.3952 - val_loss: 1.6031 - val_mae: 1.6031 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4400 - mae: 1.4400 - val_loss: 1.5834 - val_mae: 1.5834 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4086 - mae: 1.4086 - val_loss: 1.5519 - val_mae: 1.5519 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4064 - mae: 1.4064 - val_loss: 1.5538 - val_mae: 1.5538 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4103 - mae: 1.4103 - val_loss: 1.6384 - val_mae: 1.6384 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4212 - mae: 1.4212 - val_loss: 1.5367 - val_mae: 1.5367 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4234 - mae: 1.4234 - val_loss: 1.5550 - val_mae: 1.5550 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4208 - mae: 1.4208 - val_loss: 1.5430 - val_mae: 1.5430 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3939 - mae: 1.3939 - val_loss: 1.5340 - val_mae: 1.5340 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4107 - mae: 1.4107 - val_loss: 1.5418 - val_mae: 1.5418 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4051 - mae: 1.4051 - val_loss: 1.5410 - val_mae: 1.5410 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4233 - mae: 1.4233 - val_loss: 1.5335 - val_mae: 1.5335 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3986 - mae: 1.3986 - val_loss: 1.5448 - val_mae: 1.5448 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4087 - mae: 1.4087 - val_loss: 1.5391 - val_mae: 1.5391 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4011 - mae: 1.4011 - val_loss: 1.5432 - val_mae: 1.5432 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4364 - mae: 1.4364 - val_loss: 1.5557 - val_mae: 1.5557 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4161 - mae: 1.4161 - val_loss: 1.5627 - val_mae: 1.5627 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4250 - mae: 1.4250 - val_loss: 1.5551 - val_mae: 1.5551 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4123 - mae: 1.4123 - val_loss: 1.5654 - val_mae: 1.5654 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4078 - mae: 1.4078 - val_loss: 1.5557 - val_mae: 1.5557 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4139 - mae: 1.4139 - val_loss: 1.5400 - val_mae: 1.5400 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4164 - mae: 1.4164 - val_loss: 1.5591 - val_mae: 1.5591 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3956 - mae: 1.3956 - val_loss: 1.5331 - val_mae: 1.5331 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3995 - mae: 1.3995 - val_loss: 1.5475 - val_mae: 1.5475 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4115 - mae: 1.4115 - val_loss: 1.5412 - val_mae: 1.5412 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4198 - mae: 1.4198 - val_loss: 1.5661 - val_mae: 1.5661 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3982 - mae: 1.3982 - val_loss: 1.5928 - val_mae: 1.5928 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4054 - mae: 1.4054 - val_loss: 1.5409 - val_mae: 1.5409 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4004 - mae: 1.4004 - val_loss: 1.5988 - val_mae: 1.5988 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4128 - mae: 1.4128 - val_loss: 1.5683 - val_mae: 1.5683 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3868 - mae: 1.3868 - val_loss: 1.5408 - val_mae: 1.5408 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4249 - mae: 1.4249 - val_loss: 1.5466 - val_mae: 1.5466 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4097 - mae: 1.4097 - val_loss: 1.5526 - val_mae: 1.5526 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3887 - mae: 1.3887 - val_loss: 1.5607 - val_mae: 1.5607 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4146 - mae: 1.4146 - val_loss: 1.5396 - val_mae: 1.5396 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4169 - mae: 1.4169 - val_loss: 1.5419 - val_mae: 1.5419 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4144 - mae: 1.4144 - val_loss: 1.5543 - val_mae: 1.5543 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4107 - mae: 1.4107 - val_loss: 1.5449 - val_mae: 1.5449 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4341 - mae: 1.4341 - val_loss: 1.5597 - val_mae: 1.5597 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3922 - mae: 1.3922 - val_loss: 1.5423 - val_mae: 1.5423 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4071 - mae: 1.4071 - val_loss: 1.5464 - val_mae: 1.5464 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4038 - mae: 1.4038 - val_loss: 1.5828 - val_mae: 1.5828 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3986 - mae: 1.3986 - val_loss: 1.5902 - val_mae: 1.5902 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4105 - mae: 1.4105 - val_loss: 1.5448 - val_mae: 1.5448 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3812 - mae: 1.3812 - val_loss: 1.5759 - val_mae: 1.5759 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4196 - mae: 1.4196 - val_loss: 1.5416 - val_mae: 1.5416 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4129 - mae: 1.4129 - val_loss: 1.5584 - val_mae: 1.5584 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4132 - mae: 1.4132 - val_loss: 1.5384 - val_mae: 1.5384 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3962 - mae: 1.3962 - val_loss: 1.5460 - val_mae: 1.5460 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4122 - mae: 1.4122 - val_loss: 1.5412 - val_mae: 1.5412 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4088 - mae: 1.4088 - val_loss: 1.5494 - val_mae: 1.5494 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4088 - mae: 1.4088 - val_loss: 1.5738 - val_mae: 1.5738 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4033 - mae: 1.4033 - val_loss: 1.5563 - val_mae: 1.5563 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3909 - mae: 1.3909 - val_loss: 1.5490 - val_mae: 1.5490 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4071 - mae: 1.4071 - val_loss: 1.5554 - val_mae: 1.5554 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4063 - mae: 1.4063 - val_loss: 1.5886 - val_mae: 1.5886 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4071 - mae: 1.4071 - val_loss: 1.5811 - val_mae: 1.5811 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3839 - mae: 1.3839 - val_loss: 1.5507 - val_mae: 1.5507 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3949 - mae: 1.3949 - val_loss: 1.5463 - val_mae: 1.5463 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4162 - mae: 1.4162 - val_loss: 1.5545 - val_mae: 1.5545 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4243 - mae: 1.4243 - val_loss: 1.5390 - val_mae: 1.5390 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4016 - mae: 1.4016 - val_loss: 1.5535 - val_mae: 1.5535 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4071 - mae: 1.4071 - val_loss: 1.5405 - val_mae: 1.5405 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4043 - mae: 1.4043 - val_loss: 1.5381 - val_mae: 1.5381 - lr: 2.0000e-04\n",
      "Epoch 274/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3887 - mae: 1.3887 - val_loss: 1.5398 - val_mae: 1.5398 - lr: 2.0000e-04\n",
      "Epoch 275/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3623 - mae: 1.3623 - val_loss: 1.5381 - val_mae: 1.5381 - lr: 2.0000e-04\n",
      "Epoch 276/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3746 - mae: 1.3746 - val_loss: 1.5434 - val_mae: 1.5434 - lr: 2.0000e-04\n",
      "Epoch 277/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3898 - mae: 1.3898 - val_loss: 1.5385 - val_mae: 1.5385 - lr: 2.0000e-04\n",
      "Epoch 278/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4033 - mae: 1.4033 - val_loss: 1.5373 - val_mae: 1.5373 - lr: 2.0000e-04\n",
      "Epoch 279/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3899 - mae: 1.3899 - val_loss: 1.5370 - val_mae: 1.5370 - lr: 2.0000e-04\n",
      "Epoch 280/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3707 - mae: 1.3707 - val_loss: 1.5394 - val_mae: 1.5394 - lr: 2.0000e-04\n",
      "Epoch 281/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4047 - mae: 1.4047 - val_loss: 1.5355 - val_mae: 1.5355 - lr: 2.0000e-04\n",
      "Epoch 282/1000\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3780 - mae: 1.3780 - val_loss: 1.5356 - val_mae: 1.5356 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a0387d21c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 딥러닝 모델 선언\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=len(train_X.columns), activation='elu'))\n",
    "model.add(Dense(32, activation='elu'))    \n",
    "model.add(Dense(64, activation='elu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_absolute_error',\n",
    "              optimizer='Nadam', \n",
    "              metrics=['mae'])\n",
    "\n",
    "# 모델 저장 폴더 만들기\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "# 모델 업데이트 및 저장\n",
    "cp = ModelCheckpoint(filepath=modelpath, monitor='val_mae', verbose=0, save_best_only=True, mode = 'min')\n",
    "\n",
    "# 학습 자동 중단 설정\n",
    "es = EarlyStopping(monitor='val_mae', patience=50, mode='min')\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=40, mode='min')\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_X, train_y, validation_split=0.3, epochs=1000, batch_size=32, verbose=1, callbacks=[es, cp, rlrp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cde187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction = model.predict(test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6787fba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.3374815, 14.379316 ,  5.3089848, ...,  9.090191 ,  8.77253  ,\n",
       "       12.030766 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission['Target'] = np.round(Y_prediction)\n",
    "\n",
    "submission\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
